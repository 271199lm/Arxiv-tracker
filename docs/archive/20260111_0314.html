<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-11 03:14</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260111_0314</div>
    <div class="row"><div class="card">
<div class="title">Pixel-Perfect Visual Geometry Estimation</div>
<div class="meta-line">Authors: Gangwei Xu, Haotong Lin, Hongcheng Luo, Haiyang Sun, Bing Wang, Guang Chen, Sida Peng, Hangjun Ye, Xin Yang</div>
<div class="meta-line">First: 2026-01-08T18:59:49+00:00 · Latest: 2026-01-08T18:59:49+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/gangweix/pixel-perfect-depth</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05246v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05246v1">PDF</a> · <a href="https://github.com/gangweix/pixel-perfect-depth">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recovering clean and accurate geometry from images is essential for robotics and augmented reality.</div>
</details>
</div>
<div class="card">
<div class="title">SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning</div>
<div class="meta-line">Authors: Yanchang Liang, Xiaowei Zhao</div>
<div class="meta-line">First: 2026-01-08T18:10:35+00:00 · Latest: 2026-01-08T18:10:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05187v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05187v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored.</div>
</details>
</div>
<div class="card">
<div class="title">VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice</div>
<div class="meta-line">Authors: Shuming Liu, Mingchen Zhuge, Changsheng Zhao, Jun Chen, Lemeng Wu, Zechun Liu, Chenchen Zhu, Zhipeng Cai, Chong Zhou, Haozhe Liu, Ernie Chang, Saksham Suri, Hongyu Xu, Qi Qian, Wei Wen, Balakrishnan Varadarajan, Zhuang Liu, Hu Xu, Florian Bordes, Raghuraman Krishnamoorthi, Bernard Ghanem, Vikas Chandra, Yunyang Xiong</div>
<div class="meta-line">First: 2026-01-08T18:00:59+00:00 · Latest: 2026-01-08T18:00:59+00:00</div>
<div class="meta-line">Comments: Project page: https://ivul-kaust.github.io/projects/videoauto-r1/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05175v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05175v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ivul-kaust.github.io/projects/videoauto-r1/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks.</div>
</details>
</div>
<div class="card">
<div class="title">RelayLLM: Efficient Reasoning via Collaborative Decoding</div>
<div class="meta-line">Authors: Chengsong Huang, Tong Zheng, Langlin Huang, Jinyuan Li, Haolin Liu, Jiaxin Huang</div>
<div class="meta-line">First: 2026-01-08T17:56:16+00:00 · Latest: 2026-01-08T17:56:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05167v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05167v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively &quot;relaying&quot; the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity.</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Scale Local Speculative Decoding for Image Generation</div>
<div class="meta-line">Authors: Elia Peruzzo, Guillaume Sautière, Amirhossein Habibian</div>
<div class="meta-line">First: 2026-01-08T17:39:35+00:00 · Latest: 2026-01-08T17:39:35+00:00</div>
<div class="meta-line">Comments: Project page is available at https://qualcomm-ai-research.github.io/mulo-sd-webpage</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05149v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05149v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://qualcomm-ai-research.github.io/mulo-sd-webpage">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to $\mathbf{1.7\times}$ - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints.</div>
</details>
</div>
<div class="card">
<div class="title">Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models</div>
<div class="meta-line">Authors: Shuliang Liu, Xingyu Li, Hongyi Liu, Yibo Yan, Bingchen Duan, Qi Zheng, Dong Fang, Lingfeng Su, Xuming Hu</div>
<div class="meta-line">First: 2026-01-08T17:32:22+00:00 · Latest: 2026-01-08T17:32:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05144v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05144v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning Large Language Models (RLLMs) excelling in complex tasks present unique challenges for digital watermarking, as existing methods often disrupt logical coherence or incur high computational costs. Token-based watermarking techniques can corrupt the reasoning flow by applying pseudo-random biases, while semantic-aware approaches improve quality but introduce significant latency or require auxiliary models. This paper introduces ReasonMark, a novel watermarking framework specifically designed for reasoning-intensive LLMs. Our approach decouples generation into an undisturbed Thinking Phase and a watermarked Answering Phase. We propose a Criticality Score to identify semantically pivotal tokens from the reasoning trace, which are distilled into a Principal Semantic Vector (PSV). The PSV then guides a semantically-adaptive mechanism that modulates watermark strength based on token-PSV alignment, ensuring robustness without compromising logical integrity. Extensive experiments show ReasonMark surpasses state-of-the-art methods by reducing text Perplexity by 0.35, increasing translation BLEU score by 0.164, and raising mathematical accuracy by 0.67 points. These advancements are achieved alongside a 0.34% higher watermark detection AUC and stronger robustness to attacks, all with a negligible increase in latency. This work enables the traceable and trustworthy deployment of reasoning LLMs in real-world applications.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reasoning Large Language Models (RLLMs) excelling in complex tasks present unique challenges for digital watermarking, as existing methods often disrupt logical coherence or incur high computational costs.</div>
</details>
</div>
<div class="card">
<div class="title">A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering</div>
<div class="meta-line">Authors: Md. Zahid Hossain, Most. Sharmin Sultana Samu, Md. Rakibul Islam, Md. Siam Ansary</div>
<div class="meta-line">First: 2026-01-08T17:31:09+00:00 · Latest: 2026-01-08T17:31:09+00:00</div>
<div class="meta-line">Comments: Preprint, manuscript is under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05143v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05143v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation.</div>
</details>
</div>
<div class="card">
<div class="title">GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts</div>
<div class="meta-line">Authors: Wenhao Zeng, Xuteng Zhang, Yuling Shi, Chao Hu, Yuting Chen, Beijun Shen, Xiaodong Gu</div>
<div class="meta-line">First: 2026-01-08T16:58:07+00:00 · Latest: 2026-01-08T16:58:07+00:00</div>
<div class="meta-line">Comments: Code available at https://github.com/Zengwh02/GlimpRouter</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05110v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05110v1">PDF</a> · <a href="https://github.com/Zengwh02/GlimpRouter">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) achieve remarkable performance by explicitly generating multi-step chains of thought, but this capability incurs substantial inference latency and computational cost. Collaborative inference offers a promising solution by selectively allocating work between lightweight and large models, yet a fundamental challenge remains: determining when a reasoning step requires the capacity of a large model or the efficiency of a small model. Existing routing strategies either rely on local token probabilities or post-hoc verification, introducing significant inference overhead. In this work, we propose a novel perspective on step-wise collaboration: the difficulty of a reasoning step can be inferred from its very first token. Inspired by the &quot;Aha Moment&quot; phenomenon in LRMs, we show that the entropy of the initial token serves as a strong predictor of step difficulty. Building on this insight, we introduce GlimpRouter, a training-free step-wise collaboration framework. GlimpRouter employs a lightweight model to generate only the first token of each reasoning step and routes the step to a larger model only when the initial token entropy exceeds a threshold. Experiments on multiple benchmarks demonstrate that our approach significantly reduces inference latency while preserving accuracy. For instance, GlimpRouter attains a substantial 10.7% improvement in accuracy while reducing inference latency by 25.9% compared to a standalone large model on AIME25. These results suggest a simple yet effective mechanism for reasoning: allocating computation based on a glimpse of thought rather than full-step evaluation.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Reasoning Models (LRMs) achieve remarkable performance by explicitly generating multi-step chains of thought, but this capability incurs substantial inference latency and computational cost.</div>
</details>
</div>
<div class="card">
<div class="title">Token-Level LLM Collaboration via FusionRoute</div>
<div class="meta-line">Authors: Nuoya Xiong, Yuhang Zhou, Hanqing Zeng, Zhaorun Chen, Furong Huang, Shuchao Bi, Lizhu Zhang, Zhuokai Zhao</div>
<div class="meta-line">First: 2026-01-08T16:53:16+00:00 · Latest: 2026-01-08T16:53:16+00:00</div>
<div class="meta-line">Comments: 25 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05106v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05106v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert&#x27;s next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) exhibit strengths across diverse domains.</div>
</details>
</div>
<div class="card">
<div class="title">Code-Mix Sentiment Analysis on Hinglish Tweets</div>
<div class="meta-line">Authors: Aashi Garg, Aneshya Das, Arshi Arya, Anushka Goyal, Aditi</div>
<div class="meta-line">First: 2026-01-08T16:39:26+00:00 · Latest: 2026-01-08T16:39:26+00:00</div>
<div class="meta-line">Comments: Accepted at the 9th International Conference on Natural Language Processing and Information Retrieval (NLPIR 2025), Fukuoka, Japan</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05091v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05091v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish--a hybrid of Hindi and English--used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish--a hybrid of Hindi and English--used widely in user-generated content on platforms like Twitter.</div>
</details>
</div>
<div class="card">
<div class="title">Driving on Registers</div>
<div class="meta-line">Authors: Ellington Kirby, Alexandre Boulch, Yihong Xu, Yuan Yin, Gilles Puy, Éloi Zablocki, Andrei Bursuc, Spyros Gidaris, Renaud Marlet, Florent Bartoccioni, Anh-Quan Cao, Nermin Samet, Tuan-Hung VU, Matthieu Cord</div>
<div class="meta-line">First: 2026-01-08T16:28:24+00:00 · Latest: 2026-01-08T16:28:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05083v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05083v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present DrivoR, a simple and efficient transformer-based architecture for end-to-end autonomous driving. Our approach builds on pretrained Vision Transformers (ViTs) and introduces camera-aware register tokens that compress multi-camera features into a compact scene representation, significantly reducing downstream computation without sacrificing accuracy. These tokens drive two lightweight transformer decoders that generate and then score candidate trajectories. The scoring decoder learns to mimic an oracle and predicts interpretable sub-scores representing aspects such as safety, comfort, and efficiency, enabling behavior-conditioned driving at inference. Despite its minimal design, DrivoR outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and the photorealistic closed-loop HUGSIM benchmark. Our results show that a pure-transformer architecture, combined with targeted token compression, is sufficient for accurate, efficient, and adaptive end-to-end driving. Code and checkpoints will be made available via the project page.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present DrivoR, a simple and efficient transformer-based architecture for end-to-end autonomous driving.</div>
</details>
</div>
<div class="card">
<div class="title">Compositional Steering of Large Language Models with Steering Tokens</div>
<div class="meta-line">Authors: Gorjan Radevski, Kiril Gashteovski, Giwon Hong, Carolin Lawrence, Goran Glavaš</div>
<div class="meta-line">First: 2026-01-08T16:08:44+00:00 · Latest: 2026-01-08T16:08:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05062v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05062v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time.</div>
</details>
</div>
<div class="card">
<div class="title">How to Set the Learning Rate for Large-Scale Pre-training?</div>
<div class="meta-line">Authors: Yunhua Zhou, Shuhao Xing, Junhao Huang, Xipeng Qiu, Qipeng Guo</div>
<div class="meta-line">First: 2026-01-08T15:55:13+00:00 · Latest: 2026-01-08T15:55:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05049v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05049v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimal configuration of the learning rate (LR) is a fundamental yet formidable challenge in large-scale pre-training. Given the stringent trade-off between training costs and model performance, the pivotal question is whether the optimal LR can be accurately extrapolated from low-cost experiments. In this paper, we formalize this investigation into two distinct research paradigms: Fitting and Transfer. Within the Fitting Paradigm, we innovatively introduce a Scaling Law for search factor, effectively reducing the search complexity from O(n^3) to O(n*C_D*C_η) via predictive modeling. Within the Transfer Paradigm, we extend the principles of $μ$Transfer to the Mixture of Experts (MoE) architecture, broadening its applicability to encompass model depth, weight decay, and token horizons. By pushing the boundaries of existing hyperparameter research in terms of scale, we conduct a comprehensive comparison between these two paradigms. Our empirical results challenge the scalability of the widely adopted $μ$ Transfer in large-scale pre-training scenarios. Furthermore, we provide a rigorous analysis through the dual lenses of training stability and feature learning to elucidate the underlying reasons why module-wise parameter tuning underperforms in large-scale settings. This work offers systematic practical guidelines and a fresh theoretical perspective for optimizing industrial-level pre-training.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Optimal configuration of the learning rate (LR) is a fundamental yet formidable challenge in large-scale pre-training.</div>
</details>
</div>
<div class="card">
<div class="title">Talking with Tables for Better LLM Factual Data Interactions</div>
<div class="meta-line">Authors: Jio Oh, Geon Heo, Seungjun Oh, Hyunjin Kim, JinYeong Bak, Jindong Wang, Xing Xie, Steven Euijong Whang</div>
<div class="meta-line">First: 2024-12-22T23:31:03+00:00 · Latest: 2026-01-08T15:45:17+00:00</div>
<div class="meta-line">Comments: 20 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.17189v4">Abs</a> · <a href="https://arxiv.org/pdf/2412.17189v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) often struggle with requests related to information retrieval and data manipulation that frequently arise in real-world scenarios under multiple conditions. In this paper, we demonstrate that leveraging tabular structures in LLM interactions, is more effective than utilizing other structures for handling prevalent requests that operate over factual data. Through comprehensive evaluations across various scenarios and request types, we show that providing tabular structures yields a 40.29\% average performance gain along with better robustness and token efficiency. Through attention-value analysis, we discover that tables help LLMs better locate relevant information, explaining these improvements. Beyond tables and text, we evaluate whether (1) blending structuredness within text, such as providing templates or fixing the order of attributes, and (2) other representative structures, such as knowledge graphs and JSON are helpful. We observe that utilizing tables offers the best balance between efficiency and effectiveness. The method remains robust to task complexity and adapts to unstructured sources through text-to-table conversion. Overall, we highlight the untapped potential of tabular representations for future LLM applications.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) often struggle with requests related to information retrieval and data manipulation that frequently arise in real-world scenarios under multiple conditions.</div>
</details>
</div>
<div class="card">
<div class="title">ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG</div>
<div class="meta-line">Authors: Jianbo Li, Yi Jiang, Sendong Zhao, Bairui Hu, Haochun Wang, Bing Qin</div>
<div class="meta-line">First: 2026-01-08T15:44:52+00:00 · Latest: 2026-01-08T15:44:52+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/liunian-Jay/ArcAligner.git</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05038v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05038v1">PDF</a> · <a href="https://github.com/liunian-Jay/ArcAligner.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried &#x27;&#x27;compressing&#x27;&#x27; these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive &#x27;&#x27;gating&#x27;&#x27; system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive.</div>
</details>
</div>
<div class="card">
<div class="title">How to Set the Batch Size for Large-Scale Pre-training?</div>
<div class="meta-line">Authors: Yunhua Zhou, Junhao Huang, Shuhao Xin, Yechen Zhang, Runyu Peng, Qiping Guo, Xipeng Qiu</div>
<div class="meta-line">First: 2026-01-08T15:43:31+00:00 · Latest: 2026-01-08T15:43:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05034v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05034v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The concept of Critical Batch Size, as pioneered by OpenAI, has long served as a foundational principle for large-scale pre-training. However, with the paradigm shift towards the Warmup-Stable-Decay (WSD) learning rate scheduler, we observe that the original theoretical framework and its underlying mechanisms fail to align with new pre-training dynamics. To bridge this gap between theory and practice, this paper derives a revised E(S) relationship tailored for WSD scheduler, characterizing the trade-off between training data consumption E and steps S during pre-training. Our theoretical analysis reveals two fundamental properties of WSD-based pre-training: 1) B_min, the minimum batch size threshold required to achieve a target loss, and 2) B_opt, the optimal batch size that maximizes data efficiency by minimizing total tokens. Building upon these properties, we propose a dynamic Batch Size Scheduler. Extensive experiments demonstrate that our revised formula precisely captures the dynamics of large-scale pre-training, and the resulting scheduling strategy significantly enhances both training efficiency and final model quality.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The concept of Critical Batch Size, as pioneered by OpenAI, has long served as a foundational principle for large-scale pre-training.</div>
</details>
</div>
<div class="card">
<div class="title">Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing</div>
<div class="meta-line">Authors: Osvaldo Simeone</div>
<div class="meta-line">First: 2026-01-01T07:38:07+00:00 · Latest: 2026-01-08T15:17:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00245v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00245v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid growth of artificial intelligence (AI) has brought novel data processing and generative capabilities but also escalating energy requirements. This challenge motivates renewed interest in neuromorphic computing principles, which promise brain-like efficiency through discrete and sparse activations, recurrent dynamics, and non-linear feedback. In fact, modern AI architectures increasingly embody neuromorphic principles through heavily quantized activations, state-space dynamics, and sparse attention mechanisms. This paper elaborates on the connections between neuromorphic models, state-space models, and transformer architectures through the lens of the distinction between intra-token processing and inter-token processing. Most early work on neuromorphic AI was based on spiking neural networks (SNNs) for intra-token processing, i.e., for transformations involving multiple channels, or features, of the same vector input, such as the pixels of an image. In contrast, more recent research has explored how neuromorphic principles can be leveraged to design efficient inter-token processing methods, which selectively combine different information elements depending on their contextual relevance. Implementing associative memorization mechanisms, these approaches leverage state-space dynamics or sparse self-attention. Along with a systematic presentation of modern neuromorphic AI models through the lens of intra-token and inter-token processing, training methodologies for neuromorphic AI models are also reviewed. These range from surrogate gradients leveraging parallel convolutional processing to local learning rules based on reinforcement learning mechanisms.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid growth of artificial intelligence (AI) has brought novel data processing and generative capabilities but also escalating energy requirements.</div>
</details>
</div>
<div class="card">
<div class="title">On the Hidden Objective Biases of Group-based Reinforcement Learning</div>
<div class="meta-line">Authors: Aleksandar Fontana, Marco Simoni, Giulio Rossolini, Andrea Saracino, Paolo Mori</div>
<div class="meta-line">First: 2026-01-08T15:00:35+00:00 · Latest: 2026-01-08T15:00:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05002v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05002v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Group-based reinforcement learning methods, like Group Relative Policy Optimization (GRPO), are widely used nowadays to post-train large language models. Despite their empirical success, they exhibit structural mismatches between reward optimization and the underlying training objective. In this paper, we present a theoretical analysis of GRPO style methods by studying them within a unified surrogate formulation. This perspective reveals recurring properties that affect all the methods under analysis: (i) non-uniform group weighting induces systematic gradient biases on shared prefix tokens; (ii) interactions with the AdamW optimizer make training dynamics largely insensitive to reward scaling; and (iii) optimizer momentum can push policy updates beyond the intended clipping region under repeated optimization steps. We believe that these findings highlight fundamental limitations of current approaches and provide principled guidance for the design of future formulations.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Group-based reinforcement learning methods, like Group Relative Policy Optimization (GRPO), are widely used nowadays to post-train large language models.</div>
</details>
</div>
<div class="card">
<div class="title">AlgBench: To What Extent Do Large Reasoning Models Understand Algorithms?</div>
<div class="meta-line">Authors: Henan Sun, Kaichi Yu, Yuyao Wang, Bowen Liu, Xunkai Li, Rong-Hua Li, Nuo Chen, Jia Li</div>
<div class="meta-line">First: 2026-01-08T14:54:44+00:00 · Latest: 2026-01-08T14:54:44+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04996v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04996v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning ability has become a central focus in the advancement of Large Reasoning Models (LRMs). Although notable progress has been achieved on several reasoning benchmarks such as MATH500 and LiveCodeBench, existing benchmarks for algorithmic reasoning remain limited, failing to answer a critical question: Do LRMs truly master algorithmic reasoning? To answer this question, we propose AlgBench, an expert-curated benchmark that evaluates LRMs under an algorithm-centric paradigm.
  AlgBench consists of over 3,000 original problems spanning 27 algorithms, constructed by ACM algorithmic experts and organized under a comprehensive taxonomy, including Euclidean-structured, non-Euclidean-structured, non-optimized, local-optimized, global-optimized, and heuristic-optimized categories. Empirical evaluations on leading LRMs (e.g., Gemini-3-Pro, DeepSeek-v3.2-Speciale and GPT-o3) reveal substantial performance heterogeneity: while models perform well on non-optimized tasks (up to 92%), accuracy drops sharply to around 49% on globally optimized algorithms such as dynamic programming. Further analysis uncovers \textbf{strategic over-shifts}, wherein models prematurely abandon correct algorithmic designs due to necessary low-entropy tokens. These findings expose fundamental limitations of problem-centric reinforcement learning and highlight the necessity of an algorithm-centric training paradigm for robust algorithmic reasoning.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reasoning ability has become a central focus in the advancement of Large Reasoning Models (LRMs).</div>
</details>
</div>
<div class="card">
<div class="title">N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator</div>
<div class="meta-line">Authors: Zheyu Lin, Jirui Yang, Yukui Qiu, Hengqi Guo, Yubing Bao, Yao Guan</div>
<div class="meta-line">First: 2025-11-18T07:03:58+00:00 · Latest: 2026-01-08T14:19:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.14195v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.14195v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating the safety robustness of LLMs is critical for their deployment. However, mainstream Red Teaming methods rely on online generation and black-box output analysis. These approaches are not only costly but also suffer from feedback latency, making them unsuitable for agile diagnostics after training a new model. To address this, we propose N-GLARE (A Non-Generative, Latent Representation-Efficient LLM Safety Evaluator). N-GLARE operates entirely on the model&#x27;s latent representations, bypassing the need for full text generation. It characterizes hidden layer dynamics by analyzing the APT (Angular-Probabilistic Trajectory) of latent representations and introducing the JSS (Jensen-Shannon Separability) metric. Experiments on over 40 models and 20 red teaming strategies demonstrate that the JSS metric exhibits high consistency with the safety rankings derived from Red Teaming. N-GLARE reproduces the discriminative trends of large-scale red-teaming tests at less than 1\% of the token cost and the runtime cost, providing an efficient output-free evaluation proxy for real-time diagnostics.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Evaluating the safety robustness of LLMs is critical for their deployment.</div>
</details>
</div>
<div class="card">
<div class="title">Pruning the Unsurprising: Efficient LLM Reasoning via First-Token Surprisal</div>
<div class="meta-line">Authors: Wenhao Zeng, Yaoning Wang, Chao Hu, Yuling Shi, Chengcheng Wan, Hongyu Zhang, Xiaodong Gu</div>
<div class="meta-line">First: 2025-08-08T03:46:21+00:00 · Latest: 2026-01-08T14:09:36+00:00</div>
<div class="meta-line">Comments: Code and model available at https://github.com/Zengwh02/ASAP</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.05988v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.05988v2">PDF</a> · <a href="https://github.com/Zengwh02/ASAP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) have demonstrated remarkable capabilities by scaling up the length of Chain-of-Thought (CoT). However, excessively long reasoning traces pose substantial challenges for training cost and inference latency. While various CoT compression approaches have emerged to address this challenge, they face inherent trade-offs: token-level methods often disrupt syntactic and logical coherence, while step-level methods based on perplexity fail to reliably capture the logically critical reasoning steps because of the dilution of logical information. In this paper, we propose ASAP (Anchor-guided, SurprisAl-based Pruning), a novel coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided pruning to preserve the core reasoning structure, which efficiently reduces the search space for subsequent processing. Leveraging the insight that logical branching choices are concentrated at the onset of reasoning steps, it then enables logic-aware pruning by selecting logically essential reasoning steps based on a novel first-token surprisal metric. Finally, ASAP distills the models to autonomously generate and leverage these concise CoTs at inference time, enabling efficient reasoning. Experiments show that ASAP achieves state-of-the-art accuracy across multiple benchmarks while substantially reducing training and inference costs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Reasoning Models (LRMs) have demonstrated remarkable capabilities by scaling up the length of Chain-of-Thought (CoT).</div>
</details>
</div>
<div class="card">
<div class="title">DVD: A Robust Method for Detecting Variant Contamination in Large Language Model Evaluation</div>
<div class="meta-line">Authors: Renzhao Liang, Jingru Chen, Bo Jia, Bo Deng, Chenggang Xie, Yidong Wang, Ke Jin, Xin Wang, Linfeng Zhang, Cunxiang Wang</div>
<div class="meta-line">First: 2026-01-08T12:48:40+00:00 · Latest: 2026-01-08T12:48:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04895v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04895v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating large language models (LLMs) is increasingly confounded by \emph{variant contamination}: the training corpus contains semantically equivalent yet lexically or syntactically altered versions of test items. Unlike verbatim leakage, these paraphrased or structurally transformed variants evade existing detectors based on sampling consistency or perplexity, thereby inflating benchmark scores via memorization rather than genuine reasoning. We formalize this problem and introduce \textbf{DVD} (\textbf{D}etection via \textbf{V}ariance of generation \textbf{D}istribution), a single-sample detector that models the local output distribution induced by temperature sampling. Our key insight is that contaminated items trigger alternation between a \emph{memory-adherence} state and a \emph{perturbation-drift} state, yielding abnormally high variance in the synthetic difficulty of low-probability tokens; uncontaminated items remain in drift with comparatively smooth variance. We construct the first benchmark for variant contamination across two domains Omni-MATH and SuperGPQA by generating and filtering semantically equivalent variants, and simulate contamination via fine-tuning models of different scales and architectures (Qwen2.5 and Llama3.1). Across datasets and models, \textbf{DVD} consistently outperforms perplexity-based, Min-$k$\%++, edit-distance (CDD), and embedding-similarity baselines, while exhibiting strong robustness to hyperparameters. Our results establish variance of the generation distribution as a principled and practical fingerprint for detecting variant contamination in LLM evaluation.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Evaluating large language models (LLMs) is increasingly confounded by \emph{variant contamination}: the training corpus contains semantically equivalent yet lexically or syntactically altered versions of test items.</div>
</details>
</div>
<div class="card">
<div class="title">Token Maturation: Autoregressive Language Generation via Continuous Token Dynamics</div>
<div class="meta-line">Authors: Oshri Naparstek</div>
<div class="meta-line">Venue: ICML 2026</div>
<div class="meta-line">First: 2026-01-08T11:44:34+00:00 · Latest: 2026-01-08T11:44:34+00:00</div>
<div class="meta-line">Comments: In preperation to ICML 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04854v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04854v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive language models are conventionally defined over discrete token sequences, committing to a specific token at every generation step. This early discretization forces uncertainty to be resolved through token-level sampling, often leading to instability, repetition, and sensitivity to decoding heuristics.
  In this work, we introduce a continuous autoregressive formulation of language generation in which tokens are represented as continuous vectors that \emph{mature} over multiple update steps before being discretized. Rather than sampling tokens, the model evolves continuous token representations through a deterministic dynamical process, committing to a discrete token only when the representation has sufficiently converged. Discrete text is recovered via hard decoding, while uncertainty is maintained and resolved in the continuous space.
  We show that this maturation process alone is sufficient to produce coherent and diverse text using deterministic decoding (argmax), without reliance on token-level sampling, diffusion-style denoising, or auxiliary stabilization mechanisms. Additional perturbations, such as stochastic dynamics or history smoothing, can be incorporated naturally but are not required for the model to function.
  To our knowledge, this is the first autoregressive language model that generates text by evolving continuous token representations to convergence prior to discretization, enabling stable generation without token-level sampling.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autoregressive language models are conventionally defined over discrete token sequences, committing to a specific token at every generation step.</div>
</details>
</div>
<div class="card">
<div class="title">Breaking AR&#x27;s Sampling Bottleneck: Provable Acceleration via Diffusion Language Models</div>
<div class="meta-line">Authors: Gen Li, Changxiao Cai</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-27T16:24:20+00:00 · Latest: 2026-01-08T11:30:06+00:00</div>
<div class="meta-line">Comments: This is the full version of a paper published at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.21400v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.21400v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have emerged as a powerful paradigm for modern generative modeling, demonstrating strong potential for large language models (LLMs). Unlike conventional autoregressive (AR) models that generate tokens sequentially, diffusion models allow for parallel sampling, offering a promising path to accelerate generation and eliminate the left-to-right generation constraints. Despite their empirical success, theoretical understandings of diffusion language models remain underdeveloped. In this work, we develop convergence guarantees for diffusion language models from an information-theoretic perspective. Our analysis demonstrates that the sampling error, measured by the Kullback-Leibler (KL) divergence, decays inversely with the number of iterations $T$ and scales linearly with the mutual information between tokens in the target text sequence. Crucially, our theory covers the regime $T&lt;L$, where $L$ is the text sequence length. This justifies that high-quality samples can be generated with fewer iterations than $L$, thereby breaking the fundamental sampling bottleneck of $L$ steps required by AR models. We further establish matching upper and lower bounds, up to some constant factor, that shows the tightness of our convergence analysis. These results offer novel theoretical insights into the practical effectiveness of diffusion language models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion models have emerged as a powerful paradigm for modern generative modeling, demonstrating strong potential for large language models (LLMs).</div>
</details>
</div>
<div class="card">
<div class="title">Thinking-Based Non-Thinking: Solving the Reward Hacking Problem in Training Hybrid Reasoning Models via Reinforcement Learning</div>
<div class="meta-line">Authors: Siyuan Gan, Jiaheng Liu, Boyan Wang, Tianpei Yang, Runqing Miao, Yuyao Zhang, Fanyu Meng, Junlan Feng, Linjian Meng, Jing Huo, Yang Gao</div>
<div class="meta-line">First: 2026-01-08T10:38:41+00:00 · Latest: 2026-01-08T10:38:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04805v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04805v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) have attracted much attention due to their exceptional performance. However, their performance mainly stems from thinking, a long Chain of Thought (CoT), which significantly increase computational overhead. To address this overthinking problem, existing work focuses on using reinforcement learning (RL) to train hybrid reasoning models that automatically decide whether to engage in thinking or not based on the complexity of the query. Unfortunately, using RL will suffer the the reward hacking problem, e.g., the model engages in thinking but is judged as not doing so, resulting in incorrect rewards. To mitigate this problem, existing works either employ supervised fine-tuning (SFT), which incurs high computational costs, or enforce uniform token limits on non-thinking responses, which yields limited mitigation of the problem. In this paper, we propose Thinking-Based Non-Thinking (TNT). It does not employ SFT, and sets different maximum token usage for responses not using thinking across various queries by leveraging information from the solution component of the responses using thinking. Experiments on five mathematical benchmarks demonstrate that TNT reduces token usage by around 50% compared to DeepSeek-R1-Distill-Qwen-1.5B/7B and DeepScaleR-1.5B, while significantly improving accuracy. In fact, TNT achieves the optimal trade-off between accuracy and efficiency among all tested methods. Additionally, the probability of reward hacking problem in TNT&#x27;s responses, which are classified as not using thinking, remains below 10% across all tested datasets.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large reasoning models (LRMs) have attracted much attention due to their exceptional performance.</div>
</details>
</div>
<div class="card">
<div class="title">AgentOCR: Reimagining Agent History via Optical Self-Compression</div>
<div class="meta-line">Authors: Lang Feng, Fuchao Yang, Feng Chen, Xin Cheng, Haiyang Xu, Zhenglin Wan, Ming Yan, Bo An</div>
<div class="meta-line">First: 2026-01-08T10:10:20+00:00 · Latest: 2026-01-08T10:10:20+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04786v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04786v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\% of text-based agent performance while substantially reducing token consumption (&gt;50\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage.</div>
</details>
</div>
<div class="card">
<div class="title">When Single-Agent with Skills Replace Multi-Agent Systems and When They Fail</div>
<div class="meta-line">Authors: Xiaoxiao Li</div>
<div class="meta-line">First: 2026-01-08T09:14:26+00:00 · Latest: 2026-01-08T09:14:26+00:00</div>
<div class="meta-line">Comments: 25 pages, technical report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04748v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04748v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent AI systems have proven effective for complex reasoning. These systems are compounded by specialized agents, which collaborate through explicit communication, but incur substantial computational overhead. A natural question arises: can we achieve similar modularity benefits with a single agent that selects from a library of skills? We explore this question by viewing skills as internalized agent behaviors. From this perspective, a multi-agent system can be compiled into an equivalent single-agent system, trading inter-agent communication for skill selection. Our preliminary experiments suggest this approach can substantially reduce token usage and latency while maintaining competitive accuracy on reasoning benchmarks. However, this efficiency raises a deeper question that has received little attention: how does skill selection scale as libraries grow?
  Drawing on principles from cognitive science, we propose that LLM skill selection exhibits bounded capacity analogous to human decision-making. We investigate the scaling behavior of skill selection and observe a striking pattern. Rather than degrading gradually, selection accuracy remains stable up to a critical library size, then drops sharply, indicating a phase transition reminiscent of capacity limits in human cognition. Furthermore, we find evidence that semantic confusability among similar skills, rather than library size alone, plays a central role in this degradation. This perspective suggests that hierarchical organization, which has long helped humans manage complex choices, may similarly benefit AI systems. Our initial results with hierarchical routing support this hypothesis. This work opens new questions about the fundamental limits of semantic-based skill selection in LLMs and offers a cognitive-grounded framework and practical guidelines for designing scalable skill-based agents.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-agent AI systems have proven effective for complex reasoning.</div>
</details>
</div>
<div class="card">
<div class="title">Miner:Mining Intrinsic Mastery for Data-Efficient RL in Large Reasoning Models</div>
<div class="meta-line">Authors: Shuyang Jiang, Yuhao Wang, Ya Zhang, Yanfeng Wang, Yu Wang</div>
<div class="meta-line">First: 2026-01-08T08:52:37+00:00 · Latest: 2026-01-08T08:52:37+00:00</div>
<div class="meta-line">Comments: 22 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04731v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04731v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current critic-free RL methods for large reasoning models suffer from severe inefficiency when training on positive homogeneous prompts (where all rollouts are correct), resulting in waste of rollouts due to zero advantage estimates. We introduce a radically simple yet powerful solution to \uline{M}ine \uline{in}trinsic mast\uline{er}y (Miner), that repurposes the policy&#x27;s intrinsic uncertainty as a self-supervised reward signal, with no external supervision, auxiliary models, or additional inference cost. Our method pioneers two key innovations: (1) a token-level focal credit assignment mechanism that dynamically amplifies gradients on critical uncertain tokens while suppressing overconfident ones, and (2) adaptive advantage calibration to seamlessly integrate intrinsic and verifiable rewards. Evaluated across six reasoning benchmarks on Qwen3-4B and Qwen3-8B base models, Miner achieves state-of-the-art performance among the other four algorithms, yielding up to \textbf{4.58} absolute gains in Pass@1 and \textbf{6.66} gains in Pass@K compared to GRPO. Comparison with other methods targeted at exploration enhancement further discloses the superiority of the two newly proposed innovations. This demonstrates that latent uncertainty exploitation is both necessary and sufficient for efficient and scalable RL training of reasoning models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Current critic-free RL methods for large reasoning models suffer from severe inefficiency when training on positive homogeneous prompts (where all rollouts are correct), resulting in waste of rollouts due to zero advantage estimates.</div>
</details>
</div>
<div class="card">
<div class="title">Mining Intrinsic Rewards from LLM Hidden States for Efficient Best-of-N Sampling</div>
<div class="meta-line">Authors: Jizhou Guo, Zhaomin Wu, Hanchen Yang, Philip S. Yu</div>
<div class="meta-line">Venue: KDD 2026</div>
<div class="meta-line">First: 2025-05-18T04:00:35+00:00 · Latest: 2026-01-08T08:44:58+00:00</div>
<div class="meta-line">Comments: Accepted by KDD 2026 (Research Track). Project page: https://aster2024.github.io/swift-website/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.12225v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.12225v3">PDF</a> · <a href="https://github.com/aster2024/SWIFT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://aster2024.github.io/swift-website/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Best-of-N sampling is a powerful method for improving Large Language Model (LLM) performance, but it is often limited by its dependence on massive, text-based reward models. These models are not only computationally expensive but also data-hungry, requiring extensive labeled datasets for training. This creates a significant data challenge, as they overlook a rich, readily available data source: the LLM&#x27;s own internal hidden states. To address this data and efficiency gap, we introduce SWIFT (Simple Weighted Intrinsic Feedback Technique), a novel and lightweight method that learns a reward function directly from the rich information embedded in LLM hidden states. Operating at the token embedding level, SWIFT employs simple linear layers to effectively distinguish between preferred and dispreferred generations, eliminating the need for computationally intensive text-based modeling. Extensive experiments on standard benchmarks show that SWIFT outperforms existing baselines (12.7% higher accuracy than EurusRM-7B on MATH dataset) while using less than 0.005% of their parameters. Its robust scalability, compatibility with certain closed-source models via logit access, and ability to combine with traditional reward models for additional performance highlight SWIFT&#x27;s practical value and contribution to more efficient data-driven LLM post-training. Our code is available at https://github.com/aster2024/SWIFT .</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Best-of-N sampling is a powerful method for improving Large Language Model (LLM) performance, but it is often limited by its dependence on massive, text-based reward models.</div>
</details>
</div>
<div class="card">
<div class="title">Visual Merit or Linguistic Crutch? A Close Look at DeepSeek-OCR</div>
<div class="meta-line">Authors: Yunhao Liang, Ruixuan Ying, Bo Li, Hong Li, Kai Yan, Qingwen Li, Min Yang, Okamoto Satoshi, Zhe Cui, Shiwen Ni</div>
<div class="meta-line">First: 2026-01-07T09:01:23+00:00 · Latest: 2026-01-08T08:37:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03714v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03714v2">PDF</a> · <a href="https://github.com/dududuck00/DeepSeekOCR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">DeepSeek-OCR utilizes an optical 2D mapping approach to achieve high-ratio vision-text compression, claiming to decode text tokens exceeding ten times the input visual tokens. While this suggests a promising solution for the LLM long-context bottleneck, we investigate a critical question: &quot;Visual merit or linguistic crutch - which drives DeepSeek-OCR&#x27;s performance?&quot; By employing sentence-level and word-level semantic corruption, we isolate the model&#x27;s intrinsic OCR capabilities from its language priors. Results demonstrate that without linguistic support, DeepSeek-OCR&#x27;s performance plummets from approximately 90% to 20%. Comparative benchmarking against 13 baseline models reveals that traditional pipeline OCR methods exhibit significantly higher robustness to such semantic perturbations than end-to-end methods. Furthermore, we find that lower visual token counts correlate with increased reliance on priors, exacerbating hallucination risks. Context stress testing also reveals a total model collapse around 10,000 text tokens, suggesting that current optical compression techniques may paradoxically aggravate the long-context bottleneck. This study empirically defines DeepSeek-OCR&#x27;s capability boundaries and offers essential insights for future optimizations of the vision-text compression paradigm. We release all data, results and scripts used in this study at https://github.com/dududuck00/DeepSeekOCR.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DeepSeek-OCR utilizes an optical 2D mapping approach to achieve high-ratio vision-text compression, claiming to decode text tokens exceeding ten times the input visual tokens.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260110_0318.html">20260110_0318</a>
<a href="archive/20260109_0317.html">20260109_0317</a>
<a href="archive/20260108_0319.html">20260108_0319</a>
<a href="archive/20260107_0315.html">20260107_0315</a>
<a href="archive/20260106_0319.html">20260106_0319</a>
<a href="archive/20260105_0314.html">20260105_0314</a>
<a href="archive/20260104_0314.html">20260104_0314</a>
<a href="archive/20260103_0313.html">20260103_0313</a>
<a href="archive/20260102_0315.html">20260102_0315</a>
<a href="archive/20260101_0314.html">20260101_0314</a>
<a href="archive/20251231_0315.html">20251231_0315</a>
<a href="archive/20251230_0315.html">20251230_0315</a>
<a href="archive/20251229_0314.html">20251229_0314</a>
<a href="archive/20251228_0313.html">20251228_0313</a>
<a href="archive/20251227_0314.html">20251227_0314</a>
<a href="archive/20251226_0314.html">20251226_0314</a>
<a href="archive/20251225_0314.html">20251225_0314</a>
<a href="archive/20251224_0316.html">20251224_0316</a>
<a href="archive/20251223_0315.html">20251223_0315</a>
<a href="archive/20251222_0314.html">20251222_0314</a>
<a href="archive/20251221_0314.html">20251221_0314</a>
<a href="archive/20251220_0315.html">20251220_0315</a>
<a href="archive/20251219_0317.html">20251219_0317</a>
<a href="archive/20251218_0318.html">20251218_0318</a>
<a href="archive/20251217_0318.html">20251217_0318</a>
<a href="archive/20251216_0318.html">20251216_0318</a>
<a href="archive/20251215_0314.html">20251215_0314</a>
<a href="archive/20251214_0313.html">20251214_0313</a>
<a href="archive/20251213_0315.html">20251213_0315</a>
<a href="archive/20251212_0317.html">20251212_0317</a>
<a href="archive/20251211_0321.html">20251211_0321</a>
<a href="archive/20251210_0318.html">20251210_0318</a>
<a href="archive/20251209_0315.html">20251209_0315</a>
<a href="archive/20251208_0313.html">20251208_0313</a>
<a href="archive/20251207_0314.html">20251207_0314</a>
<a href="archive/20251206_0315.html">20251206_0315</a>
<a href="archive/20251205_0317.html">20251205_0317</a>
<a href="archive/20251203_0317.html">20251203_0317</a>
<a href="archive/20251202_0320.html">20251202_0320</a>
<a href="archive/20251201_0314.html">20251201_0314</a>
<a href="archive/20251130_0313.html">20251130_0313</a>
<a href="archive/20251129_0313.html">20251129_0313</a>
<a href="archive/20251128_0314.html">20251128_0314</a>
<a href="archive/20251127_0314.html">20251127_0314</a>
<a href="archive/20251126_0315.html">20251126_0315</a>
<a href="archive/20251125_0312.html">20251125_0312</a>
<a href="archive/20251124_0313.html">20251124_0313</a>
<a href="archive/20251123_0313.html">20251123_0313</a>
<a href="archive/20251122_0314.html">20251122_0314</a>
<a href="archive/20251121_0314.html">20251121_0314</a>
<a href="archive/20251120_0314.html">20251120_0314</a>
<a href="archive/20251119_0314.html">20251119_0314</a>
<a href="archive/20251118_0313.html">20251118_0313</a>
<a href="archive/20251117_0313.html">20251117_0313</a>
<a href="archive/20251116_0312.html">20251116_0312</a>
<a href="archive/20251115_0314.html">20251115_0314</a>
<a href="archive/20251114_0315.html">20251114_0315</a>
<a href="archive/20251113_0316.html">20251113_0316</a>
<a href="archive/20251112_0315.html">20251112_0315</a>
<a href="archive/20251111_0314.html">20251111_0314</a>
<a href="archive/20251110_0312.html">20251110_0312</a>
<a href="archive/20251109_0313.html">20251109_0313</a>
<a href="archive/20251108_0316.html">20251108_0316</a>
<a href="archive/20251107_0319.html">20251107_0319</a>
<a href="archive/20251106_0316.html">20251106_0316</a>
<a href="archive/20251105_0315.html">20251105_0315</a>
<a href="archive/20251104_0314.html">20251104_0314</a>
<a href="archive/20251103_0313.html">20251103_0313</a>
<a href="archive/20251102_0313.html">20251102_0313</a>
<a href="archive/20251101_0314.html">20251101_0314</a>
<a href="archive/20251031_0314.html">20251031_0314</a>
<a href="archive/20251030_0317.html">20251030_0317</a>
<a href="archive/20251029_0315.html">20251029_0315</a>
<a href="archive/20251028_0316.html">20251028_0316</a>
<a href="archive/20251027_0314.html">20251027_0314</a>
<a href="archive/20251026_0314.html">20251026_0314</a>
<a href="archive/20251025_0313.html">20251025_0313</a>
<a href="archive/20251024_0315.html">20251024_0315</a>
<a href="archive/20251023_0314.html">20251023_0314</a>
<a href="archive/20251022_0317.html">20251022_0317</a>
<a href="archive/20251021_0314.html">20251021_0314</a>
<a href="archive/20251020_0313.html">20251020_0313</a>
<a href="archive/20251019_0312.html">20251019_0312</a>
<a href="archive/20251018_0314.html">20251018_0314</a>
<a href="archive/20251017_0312.html">20251017_0312</a>
<a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>

<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-13 03:49</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260213_0349</div>
    <div class="row"><div class="card">
<div class="title">Just on Time: Token-Level Early Stopping for Diffusion Language Models</div>
<div class="meta-line">Authors: Zahar Kohut, Severyn Shykula, Dmytro Khamula, Mykola Vysotskyi, Taras Rumezhak, Volodymyr Karpiv</div>
<div class="meta-line">First: 2026-02-11T18:44:04+00:00 · Latest: 2026-02-11T18:44:04+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11133v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11133v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion language models generate text through iterative refinement, a process that is often computationally inefficient because many tokens reach stability long before the final denoising step. We introduce a training-free, token-level early stopping approach that identifies convergence independently at each position. Our method leverages lightweight signals derived from the model&#x27;s predictions and local context to dynamically determine when individual tokens can be finalized. This yields adaptive per-token freezing without task-specific fine-tuning, substantially reducing the total number of diffusion steps required. Across diverse benchmarks, spanning mathematical reasoning, general question answering, and scientific understanding, our approach achieves state-of-the-art efficiency gains while preserving generation quality.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion language models generate text through iterative refinement, a process that is often computationally inefficient because many tokens reach stability long before the final denoising step.</div>
</details>
</div>
<div class="card">
<div class="title">Token-Efficient Change Detection in LLM APIs</div>
<div class="meta-line">Authors: Timothée Chauvin, Clément Lalanne, Erwan Le Merrer, Jean-Michel Loubes, François Taïani, Gilles Tredan</div>
<div class="meta-line">First: 2026-02-11T17:48:29+00:00 · Latest: 2026-02-11T17:48:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11083v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11083v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Remote change detection in LLMs is a difficult problem. Existing methods are either too expensive for deployment at scale, or require initial white-box access to model weights or grey-box access to log probabilities. We aim to achieve both low cost and strict black-box operation, observing only output tokens.
  Our approach hinges on specific inputs we call Border Inputs, for which there exists more than one output top token. From a statistical perspective, optimal change detection depends on the model&#x27;s Jacobian and the Fisher information of the output distribution. Analyzing these quantities in low-temperature regimes shows that border inputs enable powerful change detection tests.
  Building on this insight, we propose the Black-Box Border Input Tracking (B3IT) scheme. Extensive in-vivo and in-vitro experiments show that border inputs are easily found for non-reasoning tested endpoints, and achieve performance on par with the best available grey-box approaches. B3IT reduces costs by $30\times$ compared to existing methods, while operating in a strict black-box setting.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Remote change detection in LLMs is a difficult problem.</div>
</details>
</div>
<div class="card">
<div class="title">Is In-Context Learning Learning?</div>
<div class="meta-line">Authors: Adrian de Wynter</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-12T17:12:04+00:00 · Latest: 2026-02-11T17:39:46+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026 -- CR version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.10414v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.10414v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In-context learning (ICL) allows some autoregressive models to solve tasks via next-token prediction and without needing further training. This has led to claims about these model&#x27;s ability to solve (learn) unseen tasks with only a few shots (exemplars) in the prompt. However, deduction does not always imply learning, as ICL does not explicitly encode a given observation. Instead, the models rely on their prior knowledge and the exemplars given, if any. We argue that, mathematically, ICL fits the definition of learning; however, its full characterisation requires empirical work. We then carry out a large-scale analysis of ICL ablating out or accounting for memorisation, pretraining, distributional shifts, and prompting style and phrasing. We find that, empirically, ICL is limited in its ability to learn and generalise to unseen tasks. Namely, in the limit where exemplars become more numerous, accuracy is insensitive to exemplar distribution, model, prompt style, and the input&#x27;s linguistic features. Instead, it deduces patterns from regularities in the prompt, which leads to distributional sensitivity, especially in prompting styles such as chain-of-thought. Given the varied accuracies and on formally similar tasks, we conclude that autoregression&#x27;s ad-hoc encoding is not a robust mechanism for learning, and suggests limited all-purpose generalisability.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In-context learning (ICL) allows some autoregressive models to solve tasks via next-token prediction and without needing further training.</div>
</details>
</div>
<div class="card">
<div class="title">MoToRec: Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation</div>
<div class="meta-line">Authors: Jialin Liu, Zhaorui Zhang, Ray C. C. Cheung</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2026-02-11T17:31:14+00:00 · Latest: 2026-02-11T17:31:14+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI 2026 (Main Track)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11062v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11062v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph neural networks (GNNs) have revolutionized recommender systems by effectively modeling complex user-item interactions, yet data sparsity and the item cold-start problem significantly impair performance, particularly for new items with limited or no interaction history. While multimodal content offers a promising solution, existing methods result in suboptimal representations for new items due to noise and entanglement in sparse data. To address this, we transform multimodal recommendation into discrete semantic tokenization. We present Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation (MoToRec), a framework centered on a sparsely-regularized Residual Quantized Variational Autoencoder (RQ-VAE) that generates a compositional semantic code of discrete, interpretable tokens, promoting disentangled representations. MoToRec&#x27;s architecture is enhanced by three synergistic components: (1) a sparsely-regularized RQ-VAE that promotes disentangled representations, (2) a novel adaptive rarity amplification that promotes prioritized learning for cold-start items, and (3) a hierarchical multi-source graph encoder for robust signal fusion with collaborative signals. Extensive experiments on three large-scale datasets demonstrate MoToRec&#x27;s superiority over state-of-the-art methods in both overall and cold-start scenarios. Our work validates that discrete tokenization provides an effective and scalable alternative for mitigating the long-standing cold-start challenge.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Graph neural networks (GNNs) have revolutionized recommender systems by effectively modeling complex user-item interactions, yet data sparsity and the item cold-start problem significantly impair performance, particularly for new items with limited or no interaction history.</div>
</details>
</div>
<div class="card">
<div class="title">GraphSeek: Next-Generation Graph Analytics with LLMs</div>
<div class="meta-line">Authors: Maciej Besta, Łukasz Jarmocik, Orest Hrycyna, Shachar Klaiman, Konrad Mączka, Robert Gerstenberger, Jürgen Müller, Piotr Nyczyk, Hubert Niewiadomski, Torsten Hoefler</div>
<div class="meta-line">First: 2026-02-11T17:20:06+00:00 · Latest: 2026-02-11T17:20:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11052v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11052v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graphs are foundational across domains but remain hard to use without deep expertise. LLMs promise accessible natural language (NL) graph analytics, yet they fail to process industry-scale property graphs effectively and efficiently: such datasets are large, highly heterogeneous, structurally complex, and evolve dynamically. To address this, we devise a novel abstraction for complex multi-query analytics over such graphs. Its key idea is to replace brittle generation of graph queries directly from NL with planning over a Semantic Catalog that describes both the graph schema and the graph operations. Concretely, this induces a clean separation between a Semantic Plane for LLM planning and broader reasoning, and an Execution Plane for deterministic, database-grade query execution over the full dataset and tool implementations. This design yields substantial gains in both token efficiency and task effectiveness even with small-context LLMs. We use this abstraction as the basis of the first LLM-enhanced graph analytics framework called GraphSeek. GraphSeek achieves substantially higher success rates (e.g., 86% over enhanced LangChain) and points toward the next generation of affordable and accessible graph analytics that unify LLM reasoning with database-grade execution over large and complex property graphs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Graphs are foundational across domains but remain hard to use without deep expertise.</div>
</details>
</div>
<div class="card">
<div class="title">Language Model Inversion through End-to-End Differentiation</div>
<div class="meta-line">Authors: Kevin Yandoka Denamganaï, Kartic Subr</div>
<div class="meta-line">First: 2026-02-11T17:14:41+00:00 · Latest: 2026-02-11T17:14:41+00:00</div>
<div class="meta-line">Comments: 24 pages, 5 figures, under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11044v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11044v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite emerging research on Language Models (LM), few approaches analyse the invertibility of LMs. That is, given a LM and a desirable target output sequence of tokens, determining what input prompts would yield the target output remains an open problem. We formulate this problem as a classical gradient-based optimisation. First, we propose a simple algorithm to achieve end-to-end differentiability of a given (frozen) LM and then find optimised prompts via gradient descent. Our central insight is to view LMs as functions operating on sequences of distributions over tokens (rather than the traditional view as functions on sequences of tokens). Our experiments and ablations demonstrate that our DLM-powered inversion can reliably and efficiently optimise prompts of lengths $10$ and $80$ for targets of length $20$, for several white-box LMs (out-of-the-box).</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Despite emerging research on Language Models (LM), few approaches analyse the invertibility of LMs.</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating Kubernetes Performance for GenAI Inference: From Automatic Speech Recognition to LLM Summarization</div>
<div class="meta-line">Authors: Sai Sindhur Malleni, Raúl Sevilla, Aleksei Vasilevskii, José Castillo Lema, André Bauer</div>
<div class="meta-line">First: 2026-02-03T15:36:08+00:00 · Latest: 2026-02-11T16:56:32+00:00</div>
<div class="meta-line">Comments: A accepted at the 17th International Conference on Performance Engineering</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04900v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.04900v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Generative AI (GenAI), particularly inference, rapidly emerges as a dominant workload category, the Kubernetes ecosystem is proactively evolving to natively support its unique demands. This industry paper demonstrates how emerging Kubernetes-native projects can be combined to deliver the benefits of container orchestration, such as scalability and resource efficiency, to complex AI workflows. We implement and evaluate an illustrative, multi-stage use case consisting of automatic speech recognition and summarization. First, we address batch inference by using Kueue to manage jobs that transcribe audio files with Whisper models and Dynamic Accelerator Slicer (DAS) to increase parallel job execution. Second, we address a discrete online inference scenario by feeding the transcripts to a Large Language Model for summarization hosted using llm-d, a novel solution utilizing the recent developments around the Kubernetes Gateway API Inference Extension (GAIE) for optimized routing of inference requests. Our findings illustrate that these complementary components (Kueue, DAS, and GAIE) form a cohesive, high-performance platform, proving Kubernetes&#x27; capability to serve as a unified foundation for demanding GenAI workloads: Kueue reduced total makespan by up to 15%; DAS shortened mean job completion time by 36\%; and GAIE working in conjunction with llm-d improved tail Time to First Token latency by up to 90% even under high loads.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As Generative AI (GenAI), particularly inference, rapidly emerges as a dominant workload category, the Kubernetes ecosystem is proactively evolving to natively support its unique demands.</div>
</details>
</div>
<div class="card">
<div class="title">ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression</div>
<div class="meta-line">Authors: Ammar Ali, Baher Mohammad, Denis Makhov, Dmitriy Shopkhoev, Magauiya Zhussip, Stamatios Lefkimmiatis</div>
<div class="meta-line">First: 2026-02-11T16:34:52+00:00 · Latest: 2026-02-11T16:34:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11008v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11008v1">PDF</a> · <a href="http://github.com/mts-ai/ROCKET/tree/main">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present ROCKET, a training-free model compression method that achieves state-of-the-art performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under a global compression budget, ROCKET comprises two key innovations: First, it formulates layer-wise compression allocation as a multi-choice knapsack problem, selecting the optimal compression level for each layer to minimize total reconstruction error while adhering to a target model size. Second, it introduces a single-step sparse matrix factorization inspired by dictionary learning: using only a small calibration set, it sparsifies weight coefficients based on activation-weights sensitivity and then updates the dictionary in closed form via least squares bypassing iterative optimization, sparse coding, or backpropagation entirely. ROCKET consistently outperforms existing compression approaches across different model architectures at 20-50\% compression rates. Notably, it retains over 90\% of the original model&#x27;s performance at 30\% compression without any fine-tuning. Moreover, when applying a light fine-tuning phase, recovery is substantially enhanced: for instance, compressing Qwen3-14B to an 8B-parameter model and healing it with just 30 million tokens yields performance nearly on par with the original Qwen3-8B. The code for ROCKET is at github.com/mts-ai/ROCKET/tree/main.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present ROCKET, a training-free model compression method that achieves state-of-the-art performance in comparison with factorization, structured-sparsification and dynamic compression baselines.</div>
</details>
</div>
<div class="card">
<div class="title">The emergence of numerical representations in communicating artificial agents</div>
<div class="meta-line">Authors: Daniela Mihai, Lucas Weber, Francesca Franzon</div>
<div class="meta-line">First: 2026-02-11T16:21:43+00:00 · Latest: 2026-02-11T16:21:43+00:00</div>
<div class="meta-line">Comments: In the Sixteenth International Conference on the Evolution of Language</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10996v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10996v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human languages provide efficient systems for expressing numerosities, but whether the sheer pressure to communicate is enough for numerical representations to arise in artificial agents, and whether the emergent codes resemble human numerals at all, remains an open question. We study two neural network-based agents that must communicate numerosities in a referential game using either discrete tokens or continuous sketches, thus exploring both symbolic and iconic representations. Without any pre-defined numeric concepts, the agents achieve high in-distribution communication accuracy in both communication channels and converge on high-precision symbol-meaning mappings. However, the emergent code is non-compositional: the agents fail to derive systematic messages for unseen numerosities, typically reusing the symbol of the highest trained numerosity (discrete), or collapsing extrapolated values onto a single sketch (continuous). We conclude that the communication pressure alone suffices for precise transmission of learned numerosities, but additional pressures are needed to yield compositional codes and generalisation abilities.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Human languages provide efficient systems for expressing numerosities, but whether the sheer pressure to communicate is enough for numerical representations to arise in artificial agents, and whether the emergent codes resemble human numerals at all, remains an open question.</div>
</details>
</div>
<div class="card">
<div class="title">Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression</div>
<div class="meta-line">Authors: Paul Saegert, Ullrich Köthe</div>
<div class="meta-line">First: 2026-02-09T16:47:00+00:00 · Latest: 2026-02-11T16:18:37+00:00</div>
<div class="meta-line">Comments: main text: 8 pages, 7 figures; appendix: 12 pages, 11 figures; code available at https://github.com/psaegert/simplipy and https://github.com/psaegert/flash-ansr; v2: Fixed rendering artifact in Figure 7; v3: Fixed Figure 3 title and formula</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08885v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.08885v3">PDF</a> · <a href="https://github.com/psaegert/simplipy">Code1</a> · <a href="https://github.com/psaegert/flash-ansr">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data.</div>
</details>
</div>
<div class="card">
<div class="title">Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility</div>
<div class="meta-line">Authors: Mengxuan Wang, Yuxin Chen, Gang Xu, Tao He, Hongjie Jiang, Ming Li</div>
<div class="meta-line">First: 2026-02-03T11:26:05+00:00 · Latest: 2026-02-11T16:06:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03402v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.03402v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision language models (VLMs) extend the reasoning capabilities of large language models (LLMs) to cross-modal settings, yet remain highly vulnerable to multimodal jailbreak attacks. Existing defenses predominantly rely on safety fine-tuning or aggressive token manipulations, incurring substantial training costs or significantly degrading utility. Recent research shows that LLMs inherently recognize unsafe content in text, and the incorporation of visual inputs in VLMs frequently dilutes risk-related signals. Motivated by this, we propose Risk Awareness Injection (RAI), a lightweight and training-free framework for safety calibration that restores LLM-like risk recognition by amplifying unsafe signals in VLMs. Specifically, RAI constructs an Unsafe Prototype Subspace from language embeddings and performs targeted modulation on selected high-risk visual tokens, explicitly activating safety-critical signals within the cross-modal feature space. This modulation restores the model&#x27;s LLM-like ability to detect unsafe content from visual inputs, while preserving the semantic integrity of original tokens for cross-modal reasoning. Extensive experiments across multiple jailbreak and utility benchmarks demonstrate that RAI substantially reduces attack success rate without compromising task performance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision language models (VLMs) extend the reasoning capabilities of large language models (LLMs) to cross-modal settings, yet remain highly vulnerable to multimodal jailbreak attacks.</div>
</details>
</div>
<div class="card">
<div class="title">Rotary Positional Embeddings as Phase Modulation: Theoretical Bounds on the RoPE Base for Long-Context Transformers</div>
<div class="meta-line">Authors: Feilong Liu</div>
<div class="meta-line">First: 2026-02-11T15:50:07+00:00 · Latest: 2026-02-11T15:50:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10959v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10959v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Rotary positional embeddings (RoPE) are widely used in large language models to encode token positions through multiplicative rotations, yet their behavior at long context lengths remains poorly characterized. In this work, we reinterpret RoPE as phase modulation applied to a bank of complex oscillators, enabling analysis through classical signal processing theory.
  Under this formulation, we derive principled lower bounds on the RoPE base parameter that are necessary to preserve positional coherence over a target context length. These include a fundamental aliasing bound, analogous to a Nyquist limit, and a DC-component stability bound that constrains phase drift in low-frequency positional modes. We further extend this analysis to deep transformers, showing that repeated rotary modulation across layers compounds angular misalignment, tightening the base requirement as depth increases.
  Complementing these results, we derive a precision-dependent upper bound on the RoPE base arising from finite floating-point resolution. Beyond this limit, incremental phase updates become numerically indistinguishable, leading to positional erasure even in the absence of aliasing. Together, the lower and upper bounds define a precision- and depth-dependent feasibility region a Goldilocks zone for long-context transformers.
  We validate the framework through a comprehensive case study of state-of-the-art models, including LLaMA, Mistral, and DeepSeek variants, showing that observed successes, failures, and community retrofits align closely with the predicted bounds. Notably, models that violate the stability bound exhibit attention collapse and long-range degradation, while attempts to scale beyond one million tokens encounter a hard precision wall independent of architecture or training.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Rotary positional embeddings (RoPE) are widely used in large language models to encode token positions through multiplicative rotations, yet their behavior at long context lengths remains poorly characterized.</div>
</details>
</div>
<div class="card">
<div class="title">Stochastic Parroting in Temporal Attention -- Regulating the Diagonal Sink</div>
<div class="meta-line">Authors: Victoria Hankemeier, Malte Hankemeier</div>
<div class="meta-line">First: 2026-02-11T15:45:34+00:00 · Latest: 2026-02-11T15:45:34+00:00</div>
<div class="meta-line">Comments: Accepted at ESANN 2026, Code: https://github.com/vicky-hnk/spatio-temp-parroting</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10956v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10956v1">PDF</a> · <a href="https://github.com/vicky-hnk/spatio-temp-parroting">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatio-temporal models analyze spatial structures and temporal dynamics, which makes them prone to information degeneration among space and time. Prior literature has demonstrated that over-squashing in causal attention or temporal convolutions creates a bias on the first tokens. To analyze whether such a bias is present in temporal attention mechanisms, we derive sensitivity bounds on the expected value of the Jacobian of a temporal attention layer. We theoretically show how off-diagonal attention scores depend on the sequence length, and that temporal attention matrices suffer a diagonal attention sink. We suggest regularization methods, and experimentally demonstrate their effectiveness.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Spatio-temporal models analyze spatial structures and temporal dynamics, which makes them prone to information degeneration among space and time.</div>
</details>
</div>
<div class="card">
<div class="title">Context-level Language Modeling by Learning Predictive Context Embeddings</div>
<div class="meta-line">Authors: Beiya Dai, Yuliang Liu, Daozheng Xue, Yunchong Song, Qipeng Guo, Kai Chen, Xinbing Wang, Bowen Zhou, Zhouhan Lin</div>
<div class="meta-line">First: 2025-10-23T07:09:45+00:00 · Latest: 2026-02-11T15:16:11+00:00</div>
<div class="meta-line">Comments: 19pages,6 figures, 13 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.20280v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.20280v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose ContextLM, a framework that implicitly learns multi-token prediction by augmenting standard pretraining with an intrinsic next-context prediction objective. ContextLM builds a language model on top of context embeddings that span multiple tokens, enabling better next-token prediction by predicting the next context. Our model is fully compatible with standard autoregressive, token-by-token evaluation paradigms (e.g., perplexity). Extensive experiments with GPT-2 and Pythia backbones (up to 1.5B parameters and 300B training tokens) reveal that ContextLM shifts the Pareto frontier of scaling laws, exhibiting superior efficiency in parameters, training tokens, and FLOPs. Our results show that ContextLM could already achieve the baseline perplexity using 39\% fewer parameters and demonstrates robust generalization improvements on extensive downstream tasks under equivalent parameter counts.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We propose ContextLM, a framework that implicitly learns multi-token prediction by augmenting standard pretraining with an intrinsic next-context prediction objective.</div>
</details>
</div>
<div class="card">
<div class="title">RELOOP: Recursive Retrieval with Multi-Hop Reasoner and Planners for Heterogeneous QA</div>
<div class="meta-line">Authors: Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D. Salim</div>
<div class="meta-line">First: 2025-10-23T12:48:18+00:00 · Latest: 2026-02-11T14:47:13+00:00</div>
<div class="meta-line">Comments: 19 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.20505v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.20505v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-augmented generation (RAG) remains brittle on multi-step questions and heterogeneous evidence sources, trading accuracy against latency and token/tool budgets. This paper introduces RELOOP, a structure aware framework using Hierarchical Sequence (HSEQ) that (i) linearize documents, tables, and knowledge graphs into a reversible hierarchical sequence with lightweight structural tags, and (ii) perform structure-aware iteration to collect just-enough evidence before answer synthesis. A Head Agent provides guidance that leads retrieval, while an Iteration Agent selects and expands HSeq via structure-respecting actions (e.g., parent/child hops, table row/column neighbors, KG relations); Finally the head agent composes canonicalized evidence to genearte the final answer, with an optional refinement loop to resolve detected contradictions. Experiments on HotpotQA (text), HybridQA/TAT-QA (table+text), and MetaQA (KG) show consistent EM/F1 gains over strong single-pass, multi-hop, and agentic RAG baselines with high efficiency. Besides, RELOOP exhibits three key advantages: (1) a format-agnostic unification that enables a single policy to operate across text, tables, and KGs without per-dataset specialization; (2) \textbf{guided, budget-aware iteration} that reduces unnecessary hops, tool calls, and tokens while preserving accuracy; and (3) evidence canonicalization for reliable QA, improving answers consistency and auditability.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Retrieval-augmented generation (RAG) remains brittle on multi-step questions and heterogeneous evidence sources, trading accuracy against latency and token/tool budgets.</div>
</details>
</div>
<div class="card">
<div class="title">SoftMatcha 2: A Fast and Soft Pattern Matcher for Trillion-Scale Corpora</div>
<div class="meta-line">Authors: Masataka Yoneda, Yusuke Matsushita, Go Kamoda, Kohei Suenaga, Takuya Akiba, Masaki Waga, Sho Yokoi</div>
<div class="meta-line">First: 2026-02-11T14:40:15+00:00 · Latest: 2026-02-11T14:40:15+00:00</div>
<div class="meta-line">Comments: Project Page &amp; Web Interface: https://softmatcha.github.io/v2/, Source Code: https://github.com/softmatcha/softmatcha2</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10908v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10908v1">PDF</a> · <a href="https://github.com/softmatcha/softmatcha2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://softmatcha.github.io/v2/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present an ultra-fast and flexible search algorithm that enables search over trillion-scale natural language corpora in under 0.3 seconds while handling semantic variations (substitution, insertion, and deletion). Our approach employs string matching based on suffix arrays that scales well with corpus size. To mitigate the combinatorial explosion induced by the semantic relaxation of queries, our method is built on two key algorithmic ideas: fast exact lookup enabled by a disk-aware design, and dynamic corpus-aware pruning. We theoretically show that the proposed method suppresses exponential growth in the search space with respect to query length by leveraging statistical properties of natural language. In experiments on FineWeb-Edu (Lozhkov et al., 2024) (1.4T tokens), we show that our method achieves significantly lower search latency than existing methods: infini-gram (Liu et al., 2024), infini-gram mini (Xu et al., 2025), and SoftMatcha (Deguchi et al., 2025). As a practical application, we demonstrate that our method identifies benchmark contamination in training corpora, unidentified by existing approaches. We also provide an online demo of fast, soft search across corpora in seven languages.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present an ultra-fast and flexible search algorithm that enables search over trillion-scale natural language corpora in under 0.3 seconds while handling semantic variations (substitution, insertion, and deletion).</div>
</details>
</div>
<div class="card">
<div class="title">Chart Specification: Structural Representations for Incentivizing VLM Reasoning in Chart-to-Code Generation</div>
<div class="meta-line">Authors: Minggui He, Mingchen Dai, Jian Zhang, Yilun Liu, Shimin Tao, Pufan Zeng, Osamu Yoshie, Yuya Ieiri</div>
<div class="meta-line">First: 2026-02-11T14:08:06+00:00 · Latest: 2026-02-11T14:08:06+00:00</div>
<div class="meta-line">Comments: under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10880v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10880v1">PDF</a> · <a href="https://github.com/Mighten/chart-specification-paper">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have shown promise in generating plotting code from chart images, yet achieving structural fidelity remains challenging. Existing approaches largely rely on supervised fine-tuning, encouraging surface-level token imitation rather than faithful modeling of underlying chart structure, which often leads to hallucinated or semantically inconsistent outputs. We propose Chart Specification, a structured intermediate representation that shifts training from text imitation to semantically grounded supervision. Chart Specification filters syntactic noise to construct a structurally balanced training set and supports a Spec-Align Reward that provides fine-grained, verifiable feedback on structural correctness, enabling reinforcement learning to enforce consistent plotting logic. Experiments on three public benchmarks show that our method consistently outperforms prior approaches. With only 3K training samples, we achieve strong data efficiency, surpassing leading baselines by up to 61.7% on complex benchmarks, and scaling to 4K samples establishes new state-of-the-art results across all evaluated metrics. Overall, our results demonstrate that precise structural supervision offers an efficient pathway to high-fidelity chart-to-code generation. Code and dataset are available at: https://github.com/Mighten/chart-specification-paper</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models (VLMs) have shown promise in generating plotting code from chart images, yet achieving structural fidelity remains challenging.</div>
</details>
</div>
<div class="card">
<div class="title">Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns</div>
<div class="meta-line">Authors: Xuemiao Zhang, Can Ren, Chengying Tu, Rongxiang Weng, Shuo Wang, Hongfei Yan, Jingang Wang, Xunliang Cai</div>
<div class="meta-line">First: 2025-09-25T13:11:35+00:00 · Latest: 2026-02-11T13:12:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21124v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.21124v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in large reasoning models for challenging mathematical reasoning has been driven by reinforcement learning (RL). Incorporating long chain-of-thought (CoT) data during mid-training has also been shown to substantially improve reasoning depth. However, current approaches often utilize CoT data indiscriminately, leaving open the critical question of which data types most effectively enhance model reasoning capabilities. In this paper, we define the foundation model&#x27;s reasoning potential for the first time as the inverse of the number of independent attempts required to correctly answer the question, which is strongly correlated with the final model performance. We then propose utilizing diverse data enriched with high-value reasoning patterns to expand the reasoning potential. Specifically, we abstract atomic reasoning patterns from CoT sequences, characterized by commonality and inductive capabilities, and use them to construct a core reference set enriched with valuable reasoning patterns. Furthermore, we propose a dual-granularity algorithm involving chains of reasoning patterns and token entropy, efficiently selecting high-value CoT data (CoTP) from the data pool that aligns with the core set, thereby training models to master reasoning effectively. Only 10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of downstream RL performance by 7.81%.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent progress in large reasoning models for challenging mathematical reasoning has been driven by reinforcement learning (RL).</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Confidence: The Rhythms of Reasoning in Generative Models</div>
<div class="meta-line">Authors: Deyuan Liu, Zecheng Wang, Zhanyue Qin, Zhiying Tu, Dianhui Chu, Dianbo Sui</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-11T12:58:23+00:00 · Latest: 2026-02-11T12:58:23+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10816v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10816v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) exhibit impressive capabilities yet suffer from sensitivity to slight input context variations, hampering reliability. Conventional metrics like accuracy and perplexity fail to assess local prediction robustness, as normalized output probabilities can obscure the underlying resilience of an LLM&#x27;s internal state to perturbations. We introduce the Token Constraint Bound ($δ_{\mathrm{TCB}}$), a novel metric that quantifies the maximum internal state perturbation an LLM can withstand before its dominant next-token prediction significantly changes. Intrinsically linked to output embedding space geometry, $δ_{\mathrm{TCB}}$ provides insights into the stability of the model&#x27;s internal predictive commitment. Our experiments show $δ_{\mathrm{TCB}}$ correlates with effective prompt engineering and uncovers critical prediction instabilities missed by perplexity during in-context learning and text generation. $δ_{\mathrm{TCB}}$ offers a principled, complementary approach to analyze and potentially improve the contextual stability of LLM predictions.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) exhibit impressive capabilities yet suffer from sensitivity to slight input context variations, hampering reliability.</div>
</details>
</div>
<div class="card">
<div class="title">Deep Learning-based Method for Expressing Knowledge Boundary of Black-Box LLM</div>
<div class="meta-line">Authors: Haotian Sheng, Heyong Wang, Ming Hong, Hongman He, Junqiu Liu</div>
<div class="meta-line">First: 2026-02-11T12:42:59+00:00 · Latest: 2026-02-11T12:42:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10801v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10801v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have achieved remarkable success, however, the emergence of content generation distortion (hallucination) limits their practical applications. The core cause of hallucination lies in LLMs&#x27; lack of awareness regarding their stored internal knowledge, preventing them from expressing their knowledge state on questions beyond their internal knowledge boundaries, as humans do. However, existing research on knowledge boundary expression primarily focuses on white-box LLMs, leaving methods suitable for black-box LLMs which offer only API access without revealing internal parameters-largely unexplored. Against this backdrop, this paper proposes LSCL (LLM-Supervised Confidence Learning), a deep learning-based method for expressing the knowledge boundaries of black-box LLMs. Based on the knowledge distillation framework, this method designs a deep learning model. Taking the input question, output answer, and token probability from a black-box LLM as inputs, it constructs a mapping between the inputs and the model&#x27; internal knowledge state, enabling the quantification and expression of the black-box LLM&#x27; knowledge boundaries. Experiments conducted on diverse public datasets and with multiple prominent black-box LLMs demonstrate that LSCL effectively assists black-box LLMs in accurately expressing their knowledge boundaries. It significantly outperforms existing baseline models on metrics such as accuracy and recall rate. Furthermore, considering scenarios where some black-box LLMs do not support access to token probability, an adaptive alternative method is proposed. The performance of this alternative approach is close to that of LSCL and surpasses baseline models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) have achieved remarkable success, however, the emergence of content generation distortion (hallucination) limits their practical applications.</div>
</details>
</div>
<div class="card">
<div class="title">Defect-aware Hybrid Prompt Optimization via Progressive Tuning for Zero-Shot Multi-type Anomaly Detection and Segmentation</div>
<div class="meta-line">Authors: Nadeem Nazer, Hongkuan Zhou, Lavdim Halilaj, Ylli Sadikaj, Steffen Staab</div>
<div class="meta-line">First: 2025-12-10T09:19:17+00:00 · Latest: 2026-02-11T12:13:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.09446v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.09446v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent vision language models (VLMs) like CLIP have demonstrated impressive anomaly detection performance under significant distribution shift by utilizing high-level semantic information through text prompts. However, these models often neglect fine-grained details, such as which kind of anomalies, like &quot;hole&quot;, &quot;cut&quot;, &quot;scratch&quot; that could provide more specific insight into the nature of anomalies. We argue that recognizing fine-grained anomaly types 1) enriches the representation of &quot;abnormal&quot; with structured semantics, narrowing the gap between coarse anomaly signals and fine-grained defect categories; 2) enables manufacturers to understand the root causes of the anomaly and implement more targeted and appropriate corrective measures quickly. While incorporating such detailed semantic information is crucial, designing handcrafted prompts for each defect type is both time-consuming and susceptible to human bias. For this reason, we introduce DAPO, a novel approach for Defect-aware Prompt Optimization based on progressive tuning for the zero-shot multi-type and binary anomaly detection and segmentation under distribution shifts. Our approach aligns anomaly-relevant image features with their corresponding text semantics by learning hybrid defect-aware prompts with both fixed textual anchors and learnable token embeddings. We conducted experiments on public benchmarks (MPDD, VisA, MVTec-AD, MAD, and Real-IAD) and an internal dataset. The results suggest that compared to the baseline models, DAPO achieves a 3.7% average improvement in AUROC and average precision metrics at the image level under distribution shift, and a 6.5% average improvement in localizing novel anomaly types under zero-shot settings.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent vision language models (VLMs) like CLIP have demonstrated impressive anomaly detection performance under significant distribution shift by utilizing high-level semantic information through text prompts.</div>
</details>
</div>
<div class="card">
<div class="title">Kelix Technique Report</div>
<div class="meta-line">Authors: Boyang Ding, Chenglong Chu, Dunju Zang, Han Li, Jiangxia Cao, Kun Gai, Muhao Wei, Ruiming Tang, Shiyao Wang, Siyang Mao, Xinchen Luo, Yahui Liu, Zhixin Ling, Zhuoran Yang, Ziming Li, Chengru Song, Guorui Zhou, Guowang Zhang, Hao Peng, Hao Wang, Jiaxin Deng, Jin Ouyang, Jinghao Zhang, Lejian Ren, Qianqian Wang, Qigen Hu, Tao Wang, Xingmei Wang, Yiping Yang, Zixing Zhang, Ziqi Wang</div>
<div class="meta-line">First: 2026-02-10T14:48:26+00:00 · Latest: 2026-02-11T12:04:36+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09843v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.09843v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive large language models (LLMs) scale well by expressing diverse tasks as sequences of discrete natural-language tokens and training with next-token prediction, which unifies comprehension and generation under self-supervision. Extending this paradigm to multimodal data requires a shared, discrete representation across modalities. However, most vision-language models (VLMs) still rely on a hybrid interface: discrete text tokens paired with continuous Vision Transformer (ViT) features. Because supervision is largely text-driven, these models are often biased toward understanding and cannot fully leverage large-scale self-supervised learning on non-text data. Recent work has explored discrete visual tokenization to enable fully autoregressive multimodal modeling, showing promising progress toward unified understanding and generation. Yet existing discrete vision tokens frequently lose information due to limited code capacity, resulting in noticeably weaker understanding than continuous-feature VLMs. We present Kelix, a fully discrete autoregressive unified model that closes the understanding gap between discrete and continuous visual representations.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autoregressive large language models (LLMs) scale well by expressing diverse tasks as sequences of discrete natural-language tokens and training with next-token prediction, which unifies comprehension and generation under self-supervision.</div>
</details>
</div>
<div class="card">
<div class="title">Kalman Linear Attention: Parallel Bayesian Filtering For Efficient Language Modelling and State Tracking</div>
<div class="meta-line">Authors: Vaisakh Shaj, Cameron Barker, Aidan Scannell, Andras Szecsenyi, Elliot J. Crowley, Amos Storkey</div>
<div class="meta-line">First: 2026-02-11T11:11:45+00:00 · Latest: 2026-02-11T11:11:45+00:00</div>
<div class="meta-line">Comments: Preprint. A version of this work was accepted and presented at the 1st Workshop on Epistemic Intelligence in Machine Learning (EIML) at EurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10743v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10743v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-space language models such as Mamba and gated linear attention (GLA) offer efficient alternatives to transformers due to their linear complexity and parallel training, but often lack the expressivity and robust state-tracking needed for complex reasoning. We address these limitations by reframing sequence modelling through a probabilistic lens, using Bayesian filters as a core primitive. While classical filters such as Kalman filters provide principled state estimation and uncertainty tracking, they are typically viewed as inherently sequential. We show that reparameterising the Kalman filter in information form enables its updates to be computed via an associative scan, allowing efficient parallel training. Building on this insight, we introduce the Kalman Linear Attention (KLA) layer, a neural sequence-modelling primitive that performs time-parallel probabilistic inference while maintaining explicit belief-state uncertainty. KLA offers strictly more expressive nonlinear updates and gating than GLA variants while retaining their computational advantages. On language modelling tasks, KLA matches or outperforms modern SSMs and GLAs across representative discrete token-manipulation and state-tracking benchmarks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">State-space language models such as Mamba and gated linear attention (GLA) offer efficient alternatives to transformers due to their linear complexity and parallel training, but often lack the expressivity and robust state-tracking needed for complex reasoning.</div>
</details>
</div>
<div class="card">
<div class="title">Revisit Visual Prompt Tuning: The Expressiveness of Prompt Experts</div>
<div class="meta-line">Authors: Minh Le, Anh Nguyen, Huy Nguyen, Chau Nguyen, Anh Tran, Nhat Ho</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-01-31T07:41:06+00:00 · Latest: 2026-02-11T11:10:21+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.18936v6">Abs</a> · <a href="https://arxiv.org/pdf/2501.18936v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual Prompt Tuning (VPT) has proven effective for parameter-efficient adaptation of pre-trained vision models to downstream tasks by inserting task-specific learnable prompt tokens. Despite its empirical success, a comprehensive theoretical understanding of VPT remains an active area of research. Building on the recently established connection between Mixture of Experts (MoE) and prompt-based methods, wherein each attention head can be conceptualized as a composition of multiple MoE models, we reinterpret VPT as the introduction of new prompt experts into these MoE structures. We identify a key limitation in existing VPT frameworks: the restricted functional expressiveness of prompt experts, which remain static and thus limited in their adaptability. To address this, we propose Visual Adaptive Prompt Tuning (VAPT), a novel method that endows prompt experts with enhanced expressiveness while preserving parameter efficiency. Empirical evaluations on VTAB-1K and FGVC demonstrate that VAPT achieves substantial performance improvements, surpassing fully fine-tuned baselines by 7.34% and 1.04%, respectively. Moreover, VAPT consistently outperforms VPT while requiring fewer additional parameters. Furthermore, our theoretical analysis indicates that VAPT achieves optimal sample efficiency. Collectively, these results underscore the theoretical grounding and empirical advantages of our approach.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Visual Prompt Tuning (VPT) has proven effective for parameter-efficient adaptation of pre-trained vision models to downstream tasks by inserting task-specific learnable prompt tokens.</div>
</details>
</div>
<div class="card">
<div class="title">SnapMLA: Efficient Long-Context MLA Decoding via Hardware-Aware FP8 Quantized Pipelining</div>
<div class="meta-line">Authors: Yifan Zhang, Zunhai Su, Shuhao Hu, Rui Yang, Wei Wu, Yulei Qian, Yuchen Xie, Xunliang Cai</div>
<div class="meta-line">First: 2026-02-11T10:24:42+00:00 · Latest: 2026-02-11T10:24:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10718v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10718v1">PDF</a> · <a href="https://github.com/meituan-longcat/SGLang-FluentLLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While FP8 attention has shown substantial promise in innovations like FlashAttention-3, its integration into the decoding phase of the DeepSeek Multi-head Latent Attention (MLA) architecture presents notable challenges. These challenges include numerical heterogeneity arising from the decoupling of positional embeddings, misalignment of quantization scales in FP8 PV GEMM, and the need for optimized system-level support. In this paper, we introduce SnapMLA, an FP8 MLA decoding framework optimized to improve long-context efficiency through the following hardware-aware algorithm-kernel co-optimization techniques: (i) RoPE-Aware Per-Token KV Quantization, where the RoPE part is maintained in high precision, motivated by our comprehensive analysis of the heterogeneous quantization sensitivity inherent to the MLA KV cache. Furthermore, per-token granularity is employed to align with the autoregressive decoding process and maintain quantization accuracy. (ii) Quantized PV Computation Pipeline Reconstruction, which resolves the misalignment of quantization scale in FP8 PV computation stemming from the shared KV structure of the MLA KV cache. (iii) End-to-End Dataflow Optimization, where we establish an efficient data read-and-write workflow using specialized kernels, ensuring efficient data flow and performance gains. Extensive experiments on state-of-the-art MLA LLMs show that SnapMLA achieves up to a 1.91x improvement in throughput, with negligible risk of performance degradation in challenging long-context tasks, including mathematical reasoning and code generation benchmarks. Code is available at https://github.com/meituan-longcat/SGLang-FluentLLM.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">While FP8 attention has shown substantial promise in innovations like FlashAttention-3, its integration into the decoding phase of the DeepSeek Multi-head Latent Attention (MLA) architecture presents notable challenges.</div>
</details>
</div>
<div class="card">
<div class="title">AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models</div>
<div class="meta-line">Authors: Zhifeng Rao, Wenlong Chen, Lei Xie, Xia Hua, Dongfu Yin, Zhen Tian, F. Richard Yu</div>
<div class="meta-line">First: 2026-02-11T09:57:32+00:00 · Latest: 2026-02-11T09:57:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10698v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10698v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic perception and control, yet most existing approaches primarily rely on VLM trained using 2D images, which limits their spatial understanding and action grounding in complex 3D environments. To address this limitation, we propose a novel framework that integrates depth estimation into VLA models to enrich 3D feature representations. Specifically, we employ a depth estimation baseline called VGGT to extract geometry-aware 3D cues from standard RGB inputs, enabling efficient utilization of existing large-scale 2D datasets while implicitly recovering 3D structural information. To further enhance the reliability of these depth-derived features, we introduce a new module called action assistant, which constrains the learned 3D representations with action priors and ensures their consistency with downstream control tasks. By fusing the enhanced 3D features with conventional 2D visual tokens, our approach significantly improves the generalization ability and robustness of VLA models. Experimental results demonstrate that the proposed method not only strengthens perception in geometrically ambiguous scenarios but also leads to superior action prediction accuracy. This work highlights the potential of depth-driven data augmentation and auxiliary expert supervision for bridging the gap between 2D observations and 3D-aware decision-making in robotic systems.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic perception and control, yet most existing approaches primarily rely on VLM trained using 2D images, which limits their spatial understanding and action grounding in complex 3D environments.</div>
</details>
</div>
<div class="card">
<div class="title">VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training</div>
<div class="meta-line">Authors: Guobin Shen, Chenxiao Zhao, Xiang Cheng, Lei Huang, Xing Yu</div>
<div class="meta-line">First: 2026-02-11T09:48:08+00:00 · Latest: 2026-02-11T09:48:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10693v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10693v1">PDF</a> · <a href="https://github.com/FloyedShen/VESPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs).</div>
</details>
</div>
<div class="card">
<div class="title">Variational Speculative Decoding: Rethinking Draft Training from Token Likelihood to Sequence Acceptance</div>
<div class="meta-line">Authors: Xiandong Zou, Jianshu Li, Jing Huang, Pan Zhou</div>
<div class="meta-line">First: 2026-02-05T15:36:19+00:00 · Latest: 2026-02-11T09:01:01+00:00</div>
<div class="meta-line">Comments: This paper has been withdrawn by the authors due to an error in the VSD method</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05774v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.05774v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Speculative decoding accelerates inference for (M)LLMs, yet a training-decoding discrepancy persists: while existing methods optimize single greedy trajectories, decoding involves verifying and ranking multiple sampled draft paths. We propose Variational Speculative Decoding (VSD), formulating draft training as variational inference over latent proposals (draft paths). VSD maximizes the marginal probability of target-model acceptance, yielding an ELBO that promotes high-quality latent proposals while minimizing divergence from the target distribution. To enhance quality and reduce variance, we incorporate a path-level utility and optimize via an Expectation-Maximization procedure. The E-step draws MCMC samples from an oracle-filtered posterior, while the M-step maximizes weighted likelihood using Adaptive Rejection Weighting (ARW) and Confidence-Aware Regularization (CAR). Theoretical analysis confirms that VSD increases expected acceptance length and speedup. Extensive experiments across LLMs and MLLMs show that VSD achieves up to a 9.6% speedup over EAGLE-3 and 7.9% over ViSpec, significantly improving decoding efficiency.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Speculative decoding accelerates inference for (M)LLMs, yet a training-decoding discrepancy persists: while existing methods optimize single greedy trajectories, decoding involves verifying and ranking multiple sampled draft paths.</div>
</details>
</div>
<div class="card">
<div class="title">Accelerating Streaming Video Large Language Models via Hierarchical Token Compression</div>
<div class="meta-line">Authors: Yiyu Wang, Xuyang Liu, Xiyan Gui, Xinying Lin, Boxue Yang, Chenfei Liao, Tailai Chen, Linfeng Zhang</div>
<div class="meta-line">First: 2025-11-30T13:44:28+00:00 · Latest: 2026-02-11T08:35:07+00:00</div>
<div class="meta-line">Comments: Code is avaliable at \url{https://github.com/lern-to-write/STC}</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.00891v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.00891v2">PDF</a> · <a href="https://github.com/lern-to-write/STC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \textbf{S}treaming \textbf{T}oken \textbf{C}ompression (\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \textbf{99\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \textbf{24.5\%} and \textbf{45.3\%}.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams.</div>
</details>
</div>
<div class="card">
<div class="title">OAT: Ordered Action Tokenization</div>
<div class="meta-line">Authors: Chaoqi Liu, Xiaoshen Han, Jiawei Gao, Yue Zhao, Haonan Chen, Yilun Du</div>
<div class="meta-line">First: 2026-02-04T05:01:04+00:00 · Latest: 2026-02-11T08:23:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04215v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.04215v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive policies offer a compelling foundation for scalable robot learning by enabling discrete abstraction, token-level reasoning, and flexible inference. However, applying autoregressive modeling to continuous robot actions requires an effective action tokenization scheme. Existing approaches either rely on analytical discretization methods that produce prohibitively long token sequences, or learned latent tokenizers that lack structure, limiting their compatibility with next-token prediction. In this work, we identify three desiderata for action tokenization - high compression, total decodability, and a left-to-right causally ordered token space - and introduce Ordered Action Tokenization (OAT), a learned action tokenizer that satisfies all three. OAT discretizes action chunks into an ordered sequence of tokens using transformer with registers, finite scalar quantization, and ordering-inducing training mechanisms. The resulting token space aligns naturally with autoregressive generation and enables prefix-based detokenization, yielding an anytime trade-off between inference cost and action fidelity. Across more than 20 tasks spanning four simulation benchmarks and real-world settings, autoregressive policies equipped with OAT consistently outperform prior tokenization schemes and diffusion-based baselines, while offering significantly greater flexibility at inference time.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autoregressive policies offer a compelling foundation for scalable robot learning by enabling discrete abstraction, token-level reasoning, and flexible inference.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260212_0355.html">20260212_0355</a>
<a href="archive/20260211_0358.html">20260211_0358</a>
<a href="archive/20260210_0401.html">20260210_0401</a>
<a href="archive/20260209_0322.html">20260209_0322</a>
<a href="archive/20260208_0321.html">20260208_0321</a>
<a href="archive/20260207_0336.html">20260207_0336</a>
<a href="archive/20260206_0336.html">20260206_0336</a>
<a href="archive/20260205_0337.html">20260205_0337</a>
<a href="archive/20260204_0343.html">20260204_0343</a>
<a href="archive/20260202_0322.html">20260202_0322</a>
<a href="archive/20260201_0319.html">20260201_0319</a>
<a href="archive/20260131_0329.html">20260131_0329</a>
<a href="archive/20260130_0328.html">20260130_0328</a>
<a href="archive/20260129_0325.html">20260129_0325</a>
<a href="archive/20260128_0324.html">20260128_0324</a>
<a href="archive/20260127_0321.html">20260127_0321</a>
<a href="archive/20260126_0315.html">20260126_0315</a>
<a href="archive/20260125_0315.html">20260125_0315</a>
<a href="archive/20260124_0323.html">20260124_0323</a>
<a href="archive/20260123_0322.html">20260123_0322</a>
<a href="archive/20260122_0323.html">20260122_0323</a>
<a href="archive/20260121_0410.html">20260121_0410</a>
<a href="archive/20260120_0317.html">20260120_0317</a>
<a href="archive/20260119_0313.html">20260119_0313</a>
<a href="archive/20260118_0313.html">20260118_0313</a>
<a href="archive/20260117_0319.html">20260117_0319</a>
<a href="archive/20260116_0322.html">20260116_0322</a>
<a href="archive/20260115_0318.html">20260115_0318</a>
<a href="archive/20260114_0319.html">20260114_0319</a>
<a href="archive/20260113_0319.html">20260113_0319</a>
<a href="archive/20260112_0314.html">20260112_0314</a>
<a href="archive/20260111_0314.html">20260111_0314</a>
<a href="archive/20260110_0318.html">20260110_0318</a>
<a href="archive/20260109_0317.html">20260109_0317</a>
<a href="archive/20260108_0319.html">20260108_0319</a>
<a href="archive/20260107_0315.html">20260107_0315</a>
<a href="archive/20260106_0319.html">20260106_0319</a>
<a href="archive/20260105_0314.html">20260105_0314</a>
<a href="archive/20260104_0314.html">20260104_0314</a>
<a href="archive/20260103_0313.html">20260103_0313</a>
<a href="archive/20260102_0315.html">20260102_0315</a>
<a href="archive/20260101_0314.html">20260101_0314</a>
<a href="archive/20251231_0315.html">20251231_0315</a>
<a href="archive/20251230_0315.html">20251230_0315</a>
<a href="archive/20251229_0314.html">20251229_0314</a>
<a href="archive/20251228_0313.html">20251228_0313</a>
<a href="archive/20251227_0314.html">20251227_0314</a>
<a href="archive/20251226_0314.html">20251226_0314</a>
<a href="archive/20251225_0314.html">20251225_0314</a>
<a href="archive/20251224_0316.html">20251224_0316</a>
<a href="archive/20251223_0315.html">20251223_0315</a>
<a href="archive/20251222_0314.html">20251222_0314</a>
<a href="archive/20251221_0314.html">20251221_0314</a>
<a href="archive/20251220_0315.html">20251220_0315</a>
<a href="archive/20251219_0317.html">20251219_0317</a>
<a href="archive/20251218_0318.html">20251218_0318</a>
<a href="archive/20251217_0318.html">20251217_0318</a>
<a href="archive/20251216_0318.html">20251216_0318</a>
<a href="archive/20251215_0314.html">20251215_0314</a>
<a href="archive/20251214_0313.html">20251214_0313</a>
<a href="archive/20251213_0315.html">20251213_0315</a>
<a href="archive/20251212_0317.html">20251212_0317</a>
<a href="archive/20251211_0321.html">20251211_0321</a>
<a href="archive/20251210_0318.html">20251210_0318</a>
<a href="archive/20251209_0315.html">20251209_0315</a>
<a href="archive/20251208_0313.html">20251208_0313</a>
<a href="archive/20251207_0314.html">20251207_0314</a>
<a href="archive/20251206_0315.html">20251206_0315</a>
<a href="archive/20251205_0317.html">20251205_0317</a>
<a href="archive/20251203_0317.html">20251203_0317</a>
<a href="archive/20251202_0320.html">20251202_0320</a>
<a href="archive/20251201_0314.html">20251201_0314</a>
<a href="archive/20251130_0313.html">20251130_0313</a>
<a href="archive/20251129_0313.html">20251129_0313</a>
<a href="archive/20251128_0314.html">20251128_0314</a>
<a href="archive/20251127_0314.html">20251127_0314</a>
<a href="archive/20251126_0315.html">20251126_0315</a>
<a href="archive/20251125_0312.html">20251125_0312</a>
<a href="archive/20251124_0313.html">20251124_0313</a>
<a href="archive/20251123_0313.html">20251123_0313</a>
<a href="archive/20251122_0314.html">20251122_0314</a>
<a href="archive/20251121_0314.html">20251121_0314</a>
<a href="archive/20251120_0314.html">20251120_0314</a>
<a href="archive/20251119_0314.html">20251119_0314</a>
<a href="archive/20251118_0313.html">20251118_0313</a>
<a href="archive/20251117_0313.html">20251117_0313</a>
<a href="archive/20251116_0312.html">20251116_0312</a>
<a href="archive/20251115_0314.html">20251115_0314</a>
<a href="archive/20251114_0315.html">20251114_0315</a>
<a href="archive/20251113_0316.html">20251113_0316</a>
<a href="archive/20251112_0315.html">20251112_0315</a>
<a href="archive/20251111_0314.html">20251111_0314</a>
<a href="archive/20251110_0312.html">20251110_0312</a>
<a href="archive/20251109_0313.html">20251109_0313</a>
<a href="archive/20251108_0316.html">20251108_0316</a>
<a href="archive/20251107_0319.html">20251107_0319</a>
<a href="archive/20251106_0316.html">20251106_0316</a>
<a href="archive/20251105_0315.html">20251105_0315</a>
<a href="archive/20251104_0314.html">20251104_0314</a>
<a href="archive/20251103_0313.html">20251103_0313</a>
<a href="archive/20251102_0313.html">20251102_0313</a>
<a href="archive/20251101_0314.html">20251101_0314</a>
<a href="archive/20251031_0314.html">20251031_0314</a>
<a href="archive/20251030_0317.html">20251030_0317</a>
<a href="archive/20251029_0315.html">20251029_0315</a>
<a href="archive/20251028_0316.html">20251028_0316</a>
<a href="archive/20251027_0314.html">20251027_0314</a>
<a href="archive/20251026_0314.html">20251026_0314</a>
<a href="archive/20251025_0313.html">20251025_0313</a>
<a href="archive/20251024_0315.html">20251024_0315</a>
<a href="archive/20251023_0314.html">20251023_0314</a>
<a href="archive/20251022_0317.html">20251022_0317</a>
<a href="archive/20251021_0314.html">20251021_0314</a>
<a href="archive/20251020_0313.html">20251020_0313</a>
<a href="archive/20251019_0312.html">20251019_0312</a>
<a href="archive/20251018_0314.html">20251018_0314</a>
<a href="archive/20251017_0312.html">20251017_0312</a>
<a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>

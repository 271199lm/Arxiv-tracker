<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-09 03:17</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260109_0317</div>
    <div class="row"><div class="card">
<div class="title">Attention Needs to Focus: A Unified Perspective on Attention Allocation</div>
<div class="meta-line">Authors: Zichuan Fu, Wentao Song, Guojing Li, Yejing Wang, Xian Wu, Yimin Deng, Hanyu Yan, Yefeng Zheng, Xiangyu Zhao</div>
<div class="meta-line">First: 2026-01-01T08:39:15+00:00 · Latest: 2026-01-07T18:20:49+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00919v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00919v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Transformer architecture, a cornerstone of modern Large Language Models (LLMs), has achieved extraordinary success in sequence modeling, primarily due to its attention mechanism. However, despite its power, the standard attention mechanism is plagued by well-documented issues: representational collapse and attention sink. Although prior work has proposed approaches for these issues, they are often studied in isolation, obscuring their deeper connection. In this paper, we present a unified perspective, arguing that both can be traced to a common root -- improper attention allocation. We identify two failure modes: 1) Attention Overload, where tokens receive comparable high weights, blurring semantic features that lead to representational collapse; 2) Attention Underload, where no token is semantically relevant, yet attention is still forced to distribute, resulting in spurious focus such as attention sink. Building on this insight, we introduce Lazy Attention, a novel mechanism designed for a more focused attention distribution. To mitigate overload, it employs positional discrimination across both heads and dimensions to sharpen token distinctions. To counteract underload, it incorporates Elastic-Softmax, a modified normalization function that relaxes the standard softmax constraint to suppress attention on irrelevant tokens. Experiments on the FineWeb-Edu corpus, evaluated across nine diverse benchmarks, demonstrate that Lazy Attention successfully mitigates attention sink and achieves competitive performance compared to both standard attention and modern architectures, while reaching up to 59.58% attention sparsity.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The Transformer architecture, a cornerstone of modern Large Language Models (LLMs), has achieved extraordinary success in sequence modeling, primarily due to its attention mechanism.</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning</div>
<div class="meta-line">Authors: Yifan Wang, Yanyu Li, Sergey Tulyakov, Yun Fu, Anil Kag</div>
<div class="meta-line">First: 2026-01-07T18:05:08+00:00 · Latest: 2026-01-07T18:05:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04153v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04153v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Direct Preference Optimization (DPO) has recently improved Text-to-Video (T2V) generation by enhancing visual fidelity and text alignment. However, current methods rely on non-differentiable preference signals from human annotations or learned reward models. This reliance makes training label-intensive, bias-prone, and easy-to-game, which often triggers reward hacking and unstable training. We propose Diffusion-DRF, a differentiable reward flow for fine-tuning video diffusion models using a frozen, off-the-shelf Vision-Language Model (VLM) as a training-free critic. Diffusion-DRF directly backpropagates VLM feedback through the diffusion denoising chain, converting logit-level responses into token-aware gradients for optimization. We propose an automated, aspect-structured prompting pipeline to obtain reliable multi-dimensional VLM feedback, while gradient checkpointing enables efficient updates through the final denoising steps. Diffusion-DRF improves video quality and semantic alignment while mitigating reward hacking and collapse -- without additional reward models or preference datasets. It is model-agnostic and readily generalizes to other diffusion-based generative tasks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Direct Preference Optimization (DPO) has recently improved Text-to-Video (T2V) generation by enhancing visual fidelity and text alignment.</div>
</details>
</div>
<div class="card">
<div class="title">Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction</div>
<div class="meta-line">Authors: Jiaxin Huang, Yuanbo Yang, Bangbang Yang, Lin Ma, Yuewen Ma, Yiyi Liao</div>
<div class="meta-line">First: 2026-01-07T16:57:30+00:00 · Latest: 2026-01-07T16:57:30+00:00</div>
<div class="meta-line">Comments: Project page: https://xdimlab.github.io/Gen3R/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04090v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04090v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://xdimlab.github.io/Gen3R/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and video diffusion models for scene-level 3D generation. We repurpose the VGGT reconstruction model to produce geometric latents by training an adapter on its tokens, which are regularized to align with the appearance latents of pre-trained video diffusion models. By jointly generating these disentangled yet aligned latents, Gen3R produces both RGB videos and corresponding 3D geometry, including camera poses, depth maps, and global point clouds. Experiments demonstrate that our approach achieves state-of-the-art results in single- and multi-image conditioned 3D scene generation. Additionally, our method can enhance the robustness of reconstruction by leveraging generative priors, demonstrating the mutual benefit of tightly coupling reconstruction and generative models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and video diffusion models for scene-level 3D generation.</div>
</details>
</div>
<div class="card">
<div class="title">The Mean-Field Dynamics of Transformers</div>
<div class="meta-line">Authors: Philippe Rigollet</div>
<div class="meta-line">First: 2025-12-01T16:51:00+00:00 · Latest: 2026-01-07T15:27:09+00:00</div>
<div class="meta-line">Comments: to appear as Proceedings of the ICM2026, Philadelphia, USA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.01868v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.01868v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We develop a mathematical framework that interprets Transformer attention as an interacting particle system and studies its continuum (mean-field) limits. By idealizing attention on the sphere, we connect Transformer dynamics to Wasserstein gradient flows, synchronization models (Kuramoto), and mean-shift clustering. Central to our results is a global clustering phenomenon whereby tokens cluster asymptotically after long metastable states where they are arranged into multiple clusters. We further analyze a tractable equiangular reduction to obtain exact clustering rates, show how commonly used normalization schemes alter contraction speeds, and identify a phase transition for long-context attention. The results highlight both the mechanisms that drive representation collapse and the regimes that preserve expressive, multi-cluster structure in deep attention architectures.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We develop a mathematical framework that interprets Transformer attention as an interacting particle system and studies its continuum (mean-field) limits.</div>
</details>
</div>
<div class="card">
<div class="title">DiFlow-TTS: Compact and Low-Latency Zero-Shot Text-to-Speech with Factorized Discrete Flow Matching</div>
<div class="meta-line">Authors: Ngoc-Son Nguyen, Thanh V. T. Tran, Hieu-Nghia Huynh-Nguyen, Truong-Son Hy, Van Nguyen</div>
<div class="meta-line">First: 2025-09-11T17:16:52+00:00 · Latest: 2026-01-07T14:31:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.09631v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.09631v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://diflow-tts.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces DiFlow-TTS, a novel zero-shot text-to-speech (TTS) system that employs discrete flow matching for generative speech modeling. We position this work as an entry point that may facilitate further advances in this research direction. Through extensive empirical evaluation, we analyze both the strengths and limitations of this approach across key aspects, including naturalness, expressive attributes, speaker identity, and inference latency. To this end, we leverage factorized speech representations and design a deterministic Phoneme-Content Mapper for modeling linguistic content, together with a Factorized Discrete Flow Denoiser that jointly models multiple discrete token streams corresponding to prosody and acoustics to capture expressive speech attributes. Experimental results demonstrate that DiFlow-TTS achieves strong performance across multiple metrics while maintaining a compact model size, up to 11.7 times smaller, and enabling low-latency inference that is up to 34 times faster than recent state-of-the-art baselines. Audio samples are available on our demo page: https://diflow-tts.github.io.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces DiFlow-TTS, a novel zero-shot text-to-speech (TTS) system that employs discrete flow matching for generative speech modeling.</div>
</details>
</div>
<div class="card">
<div class="title">Anti-Length Shift: Dynamic Outlier Truncation for Training Efficient Reasoning Models</div>
<div class="meta-line">Authors: Wei Wu, Liyi Chen, Congxi Xiao, Tianfu Wang, Qimeng Wang, Chengqiang Lu, Yan Gao, Yi Wu, Yao Hu, Hui Xiong</div>
<div class="meta-line">First: 2026-01-07T14:31:07+00:00 · Latest: 2026-01-07T14:31:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03969v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03969v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models enhanced by reinforcement learning with verifiable rewards have achieved significant performance gains by extending their chain-of-thought. However, this paradigm incurs substantial deployment costs as models often exhibit excessive verbosity on simple queries. Existing efficient reasoning methods relying on explicit length penalties often introduce optimization conflicts and leave the generative mechanisms driving overthinking largely unexamined. In this paper, we identify a phenomenon termed length shift where models increasingly generate unnecessary reasoning on trivial inputs during training. To address this, we introduce Dynamic Outlier Truncation (DOT), a training-time intervention that selectively suppresses redundant tokens. This method targets only the extreme tail of response lengths within fully correct rollout groups while preserving long-horizon reasoning capabilities for complex problems. To complement this intervention and ensure stable convergence, we further incorporate auxiliary KL regularization and predictive dynamic sampling. Experimental results across multiple model scales demonstrate that our approach significantly pushes the efficiency-performance Pareto frontier outward. Notably, on the AIME-24, our method reduces inference token usage by 78% while simultaneously increasing accuracy compared to the initial policy and surpassing state-of-the-art efficient reasoning methods.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large reasoning models enhanced by reinforcement learning with verifiable rewards have achieved significant performance gains by extending their chain-of-thought.</div>
</details>
</div>
<div class="card">
<div class="title">ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation</div>
<div class="meta-line">Authors: Xu Zhang, Cheng Da, Huan Yang, Kun Gai, Ming Lu, Zhan Ma</div>
<div class="meta-line">First: 2026-01-07T14:09:18+00:00 · Latest: 2026-01-07T14:09:18+00:00</div>
<div class="meta-line">Comments: Technical report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03955v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03955v1">PDF</a> · <a href="https://github.com/Kwai-Kolors/ResTok">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring &quot;vision&quot; back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams.</div>
</details>
</div>
<div class="card">
<div class="title">FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection</div>
<div class="meta-line">Authors: Mingyu Ouyang, Kevin Qinghong Lin, Mike Zheng Shou, Hwee Tou Ng</div>
<div class="meta-line">First: 2026-01-07T13:48:12+00:00 · Latest: 2026-01-07T13:48:12+00:00</div>
<div class="meta-line">Comments: 14 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03928v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03928v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have shown remarkable performance in User Interface (UI) grounding tasks, driven by their ability to process increasingly high-resolution screenshots. However, screenshots are tokenized into thousands of visual tokens (e.g., about 4700 for 2K resolution), incurring significant computational overhead and diluting attention. In contrast, humans typically focus on regions of interest when interacting with UI. In this work, we pioneer the task of efficient UI grounding. Guided by practical analysis of the task&#x27;s characteristics and challenges, we propose FocusUI, an efficient UI grounding framework that selects patches most relevant to the instruction while preserving positional continuity for precise grounding. FocusUI addresses two key challenges: (1) Eliminating redundant tokens in visual encoding. We construct patch-level supervision by fusing an instruction-conditioned score with a rule-based UI-graph score that down-weights large homogeneous regions to select distinct and instruction-relevant visual tokens. (2) Preserving positional continuity during visual token selection. We find that general visual token pruning methods suffer from severe accuracy degradation on UI grounding tasks due to broken positional information. We introduce a novel PosPad strategy, which compresses each contiguous sequence of dropped visual tokens into a single special marker placed at the sequence&#x27;s last index to preserve positional continuity. Comprehensive experiments on four grounding benchmarks demonstrate that FocusUI surpasses GUI-specific baselines. On the ScreenSpot-Pro benchmark, FocusUI-7B achieves a performance improvement of 3.7% over GUI-Actor-7B. Even with only 30% visual token retention, FocusUI-7B drops by only 3.2% while achieving up to 1.44x faster inference and 17% lower peak GPU memory.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models (VLMs) have shown remarkable performance in User Interface (UI) grounding tasks, driven by their ability to process increasingly high-resolution screenshots.</div>
</details>
</div>
<div class="card">
<div class="title">IndexTTS 2.5 Technical Report</div>
<div class="meta-line">Authors: Yunpei Li, Xun Zhou, Jinchao Wang, Lu Wang, Yong Wu, Siyi Zhou, Yiquan Zhou, Jingchen Shu</div>
<div class="meta-line">First: 2026-01-07T12:58:16+00:00 · Latest: 2026-01-07T12:58:16+00:00</div>
<div class="meta-line">Comments: 11 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03888v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03888v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In prior work, we introduced IndexTTS 2, a zero-shot neural text-to-speech foundation model comprising two core components: a transformer-based Text-to-Semantic (T2S) module and a non-autoregressive Semantic-to-Mel (S2M) module, which together enable faithful emotion replication and establish the first autoregressive duration-controllable generative paradigm. Building upon this, we present IndexTTS 2.5, which significantly enhances multilingual coverage, inference speed, and overall synthesis quality through four key improvements: 1) Semantic Codec Compression: we reduce the semantic codec frame rate from 50 Hz to 25 Hz, halving sequence length and substantially lowering both training and inference costs; 2) Architectural Upgrade: we replace the U-DiT-based backbone of the S2M module with a more efficient Zipformer-based modeling architecture, achieving notable parameter reduction and faster mel-spectrogram generation; 3) Multilingual Extension: We propose three explicit cross-lingual modeling strategies, boundary-aware alignment, token-level concatenation, and instruction-guided generation, establishing practical design principles for zero-shot multilingual emotional TTS that supports Chinese, English, Japanese, and Spanish, and enables robust emotion transfer even without target-language emotional training data; 4) Reinforcement Learning Optimization: we apply GRPO in post-training of the T2S module, improving pronunciation accuracy and natrualness. Experiments show that IndexTTS 2.5 not only supports broader language coverage but also replicates emotional prosody in unseen languages under the same zero-shot setting. IndexTTS 2.5 achieves a 2.28 times improvement in RTF while maintaining comparable WER and speaker similarity to IndexTTS 2.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In prior work, we introduced IndexTTS 2, a zero-shot neural text-to-speech foundation model comprising two core components: a transformer-based Text-to-Semantic (T2S) module and a non-autoregressive Semantic-to-Mel (S2M) module, which together enable faithful emotion replication and establish the first autoregressive duration-controllable generative paradigm.</div>
</details>
</div>
<div class="card">
<div class="title">Feature-Aware One-Shot Federated Learning via Hierarchical Token Sequences</div>
<div class="meta-line">Authors: Shudong Liu, Hanwen Zhang, Xiuling Wang, Yuesheng Zhu, Guibo Luo</div>
<div class="meta-line">First: 2026-01-07T12:48:16+00:00 · Latest: 2026-01-07T12:48:16+00:00</div>
<div class="meta-line">Comments: 9 pages; 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03882v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03882v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One-shot federated learning (OSFL) reduces the communication cost and privacy risks of iterative federated learning by constructing a global model with a single round of communication. However, most existing methods struggle to achieve robust performance on real-world domains such as medical imaging, or are inefficient when handling non-IID (Independent and Identically Distributed) data. To address these limitations, we introduce FALCON, a framework that enhances the effectiveness of OSFL over non-IID image data. The core idea of FALCON is to leverage the feature-aware hierarchical token sequences generation and knowledge distillation into OSFL. First, each client leverages a pretrained visual encoder with hierarchical scale encoding to compress images into hierarchical token sequences, which capture multi-scale semantics. Second, a multi-scale autoregressive transformer generator is used to model the distribution of these token sequences and generate the synthetic sequences. Third, clients upload the synthetic sequences along with the local classifier trained on the real token sequences to the server. Finally, the server incorporates knowledge distillation into global training to reduce reliance on precise distribution modeling. Experiments on medical and natural image datasets validate the effectiveness of FALCON in diverse non-IID scenarios, outperforming the best OSFL baselines by 9.58% in average accuracy.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">One-shot federated learning (OSFL) reduces the communication cost and privacy risks of iterative federated learning by constructing a global model with a single round of communication.</div>
</details>
</div>
<div class="card">
<div class="title">GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory</div>
<div class="meta-line">Authors: Jiaxu Liu, Yuhe Bai, Xiangyu Yin, Christos-Savvas Bouganis</div>
<div class="meta-line">First: 2025-12-08T18:11:06+00:00 · Latest: 2026-01-07T12:18:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.07782v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.07782v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern autoregressive models rely on attention, yet the Softmax full attention in Transformers scales quadratically with sequence length. Sliding Window Attention (SWA) achieves linear-time encoding/decoding by constraining the attention pattern, but under an \textit{Associative Memory} interpretation, its difference-style update renders the training objective effectively \emph{unbounded}. In contrast, Softmax attention normalizes updates, leading to \emph{memory shrinkage and gradient vanishing}. We propose GatedFWA: a Memory-\underline{Gated} (\underline{F}lash) \underline{W}indowed \underline{A}ttention mechanism that preserves SWAs efficiency while stabilizing memory updates and making gradient flow controllable. In essence, GatedFWA accumulate a per-token/head gate into a decay bias added to the attention logits, acting as a learnable contraction in the memory recurrence. We implement a fused one-pass gate preprocessing and a FlashAttention-compatible kernel that injects the gate under a sliding mask, ensuring I/O efficiency and numerical stability. On language modelling benchmarks, GatedFWA delivers competitive throughput with negligible overhead and better use of global context, and it integrates cleanly with token compression/selection methods such as NSA and generalizes to various autoregressive domains.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modern autoregressive models rely on attention, yet the Softmax full attention in Transformers scales quadratically with sequence length.</div>
</details>
</div>
<div class="card">
<div class="title">A Systematic Comparison between Extractive Self-Explanations and Human Rationales in Text Classification</div>
<div class="meta-line">Authors: Stephanie Brandl, Oliver Eberle</div>
<div class="meta-line">First: 2024-10-04T10:14:12+00:00 · Latest: 2026-01-07T12:08:07+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.03296v3">Abs</a> · <a href="https://arxiv.org/pdf/2410.03296v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Instruction-tuned LLMs are able to provide \textit{an} explanation about their output to users by generating self-explanations, without requiring the application of complex interpretability techniques. In this paper, we analyse whether this ability results in a \textit{good} explanation. We evaluate self-explanations in the form of input rationales with respect to their plausibility to humans. We study three text classification tasks: sentiment classification, forced labour detection and claim verification. We include Danish and Italian translations of the sentiment classification task and compare self-explanations to human annotations. For this, we collected human rationale annotations for Climate-Fever, a claim verification dataset. We furthermore evaluate the faithfulness of human and self-explanation rationales with respect to correct model predictions, and extend the study by incorporating post-hoc attribution-based explanations. We analyse four open-weight LLMs and find that alignment between self-explanations and human rationales highly depends on text length and task complexity. Nevertheless, self-explanations yield faithful subsets of token-level rationales, whereas post-hoc attribution methods tend to emphasize structural and formatting tokens, reflecting fundamentally different explanation strategies.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Instruction-tuned LLMs are able to provide \textit{an} explanation about their output to users by generating self-explanations, without requiring the application of complex interpretability techniques.</div>
</details>
</div>
<div class="card">
<div class="title">ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition</div>
<div class="meta-line">Authors: Muyang Zhao, Qi Qi, Hao Sun</div>
<div class="meta-line">First: 2026-01-07T11:30:55+00:00 · Latest: 2026-01-07T11:30:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03822v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03822v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation a task requires. We study budgeted inference-time reasoning for multiple tasks under a strict global token constraint and formalize it as a Ordered Stochastic Multiple-Choice Knapsack Problem(OS-MCKP). This perspective highlights a meta-cognitive requirement -- anticipating task difficulty, estimating return over investment (ROI), and allocating computation strategically. We propose ROI-Reasoning, a two-stage framework that endows LLMs with intrinsic, budget-aware rationality. In the first stage, Meta-Cognitive Fine-Tuning teaches models to predict reasoning cost and expected utility before generation, enabling explicit solve-or-skip decisions. Next, Rationality-Aware Reinforcement Learning optimizes sequential decision making under a hard token budget, allowing models to learn long-horizon allocation strategies. Across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall score while substantially reducing regret under tight computation budgets.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation a task requires.</div>
</details>
</div>
<div class="card">
<div class="title">Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents</div>
<div class="meta-line">Authors: Dehao Tao, Guoliang Ma, Yongfeng Huang, Minghu Jiang</div>
<div class="meta-line">First: 2026-01-07T10:36:29+00:00 · Latest: 2026-01-07T10:36:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03785v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03785v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it. Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval. This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity. We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent &quot;memory boxes&quot; at storage time. Sealed boxes are then linked by a Trace Weaver into long-range event-timeline traces, recovering macro-topic recurrences across discontinuities. Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM). Notably, Membox attains these gains while using only a fraction of the context tokens required by existing methods, highlighting a superior balance between efficiency and effectiveness. By explicitly modeling topic continuity, Membox offers a cognitively motivated mechanism for enhancing both coherence and efficiency in LLM agents.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it.</div>
</details>
</div>
<div class="card">
<div class="title">Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation</div>
<div class="meta-line">Authors: Yushe Cao, Dianxi Shi, Xing Fu, Xuechao Zou, Haikuo Peng, Xueqi Li, Chun Yu, Junliang Xing</div>
<div class="meta-line">First: 2025-11-16T14:52:54+00:00 · Latest: 2026-01-07T10:24:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12631v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.12631v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes.</div>
</details>
</div>
<div class="card">
<div class="title">ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder</div>
<div class="meta-line">Authors: Xiaoxing Hu, Kaicheng Yang, Ziyang Gong, Qi Ming, Zonghao Guo, Yu Tian, Xiang An, Ziyong Feng, Xue Yang</div>
<div class="meta-line">First: 2025-10-21T16:48:49+00:00 · Latest: 2026-01-07T09:27:16+00:00</div>
<div class="meta-line">Comments: 17 pages, 5 fiugres</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.18795v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.18795v3">PDF</a> · <a href="https://github.com/VisionXLab/ProCLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The original CLIP text encoder is limited by a maximum input length of 77 tokens, which hampers its ability to effectively process long texts and perform fine-grained semantic understanding. In addition, the CLIP text encoder lacks support for multilingual inputs. All these limitations significantly restrict its applicability across a broader range of tasks. Recent studies have attempted to replace the CLIP text encoder with an LLM-based embedder to enhance its ability in processing long texts, multilingual understanding, and fine-grained semantic comprehension. However, because the representation spaces of LLMs and the vision-language space of CLIP are pretrained independently without alignment priors, direct alignment using contrastive learning can disrupt the intrinsic vision-language alignment in the CLIP image encoder, leading to an underutilization of the knowledge acquired during pre-training. To address this challenge, we propose ProCLIP, a curriculum learning-based progressive vision-language alignment framework to effectively align the CLIP image encoder with an LLM-based embedder. Specifically, ProCLIP first distills knowledge from CLIP&#x27;s text encoder into the LLM-based embedder to leverage CLIP&#x27;s rich pretrained knowledge while establishing initial alignment between the LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns the CLIP image encoder with the LLM-based embedder through image-text contrastive tuning, employing self-distillation regularization to avoid overfitting. To achieve a more effective alignment, instance semantic alignment loss and embedding structure alignment loss are employed during representation inheritance and contrastive tuning. The Code is available at https://github.com/VisionXLab/ProCLIP.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The original CLIP text encoder is limited by a maximum input length of 77 tokens, which hampers its ability to effectively process long texts and perform fine-grained semantic understanding.</div>
</details>
</div>
<div class="card">
<div class="title">EDCO: Dynamic Curriculum Orchestration for Domain-specific Large Language Model Fine-tuning</div>
<div class="meta-line">Authors: Jing-Cheng Pang, Liu Sun, Chang Zhou, Xian Tang, Haichuan Ma, Kun Jiang, Jianlong Wang, Kai Zhang, Sijie Wu, Haoran Cai, Chenwei Wu, Xubin Li, Xin Chen</div>
<div class="meta-line">First: 2026-01-07T09:20:05+00:00 · Latest: 2026-01-07T09:20:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03725v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03725v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Domain-specific large language models (LLMs), typically developed by fine-tuning a pre-trained general-purpose LLM on specialized datasets, represent a significant advancement in applied AI. A common strategy in LLM fine-tuning is curriculum learning, which pre-orders training samples based on metrics like difficulty to improve learning efficiency compared to a random sampling strategy. However, most existing methods for LLM fine-tuning rely on a static curriculum, designed prior to training, which lacks adaptability to the model&#x27;s evolving needs during fine-tuning. To address this, we propose EDCO, a novel framework based on two key concepts: inference entropy and dynamic curriculum orchestration. Inspired by recent findings that maintaining high answer entropy benefits long-term reasoning gains, EDCO prioritizes samples with high inference entropy in a continuously adapted curriculum. EDCO integrates three core components: an efficient entropy estimator that uses prefix tokens to approximate full-sequence entropy, an entropy-based curriculum generator that selects data points with the highest inference entropy, and an LLM trainer that optimizes the model on the selected curriculum. Comprehensive experiments in communication, medicine and law domains, EDCO outperforms traditional curriculum strategies for fine-tuning Qwen3-4B and Llama3.2-3B models under supervised and reinforcement learning settings. Furthermore, the proposed efficient entropy estimation reduces computational time by 83.5% while maintaining high accuracy.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Domain-specific large language models (LLMs), typically developed by fine-tuning a pre-trained general-purpose LLM on specialized datasets, represent a significant advancement in applied AI.</div>
</details>
</div>
<div class="card">
<div class="title">Visual Merit or Linguistic Crutch? A Close Look at DeepSeek-OCR</div>
<div class="meta-line">Authors: Yunhao Liang, Ruixuan Ying, Bo Li, Hong Li, Kai Yan, Qingwen Li, Min Yang, Okamoto Satoshi, Zhe Cui, Shiwen Ni</div>
<div class="meta-line">First: 2026-01-07T09:01:23+00:00 · Latest: 2026-01-07T09:01:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03714v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03714v1">PDF</a> · <a href="https://github.com/dududuck00/DeepSeekOCR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">DeepSeek-OCR utilizes an optical 2D mapping approach to achieve high-ratio vision-text compression, claiming to decode text tokens exceeding ten times the input visual tokens. While this suggests a promising solution for the LLM long-context bottleneck, we investigate a critical question: &quot;Visual merit or linguistic crutch - which drives DeepSeek-OCR&#x27;s performance?&quot; By employing sentence-level and word-level semantic corruption, we isolate the model&#x27;s intrinsic OCR capabilities from its language priors. Results demonstrate that without linguistic support, DeepSeek-OCR&#x27;s performance plummets from approximately 90% to 20%. Comparative benchmarking against 13 baseline models reveals that traditional pipeline OCR methods exhibit significantly higher robustness to such semantic perturbations than end-to-end methods. Furthermore, we find that lower visual token counts correlate with increased reliance on priors, exacerbating hallucination risks. Context stress testing also reveals a total model collapse around 10,000 text tokens, suggesting that current optical compression techniques may paradoxically aggravate the long-context bottleneck. This study empirically defines DeepSeek-OCR&#x27;s capability boundaries and offers essential insights for future optimizations of the vision-text compression paradigm. We release all data, results and scripts used in this study at https://github.com/dududuck00/DeepSeekOCR.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DeepSeek-OCR utilizes an optical 2D mapping approach to achieve high-ratio vision-text compression, claiming to decode text tokens exceeding ten times the input visual tokens.</div>
</details>
</div>
<div class="card">
<div class="title">TreeAdv: Tree-Structured Advantage Redistribution for Group-Based RL</div>
<div class="meta-line">Authors: Lang Cao, Hui Ruan, Yongqian Li, Peng Chao, Wu Ning, Haonan Song, Renhong Chen, Yitong Li</div>
<div class="meta-line">First: 2026-01-07T08:42:14+00:00 · Latest: 2026-01-07T08:42:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03703v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03703v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with group-based objectives, such as Group Relative Policy Optimization (GRPO), is a common framework for aligning large language models on complex reasoning tasks. However, standard GRPO treats each rollout trajectory as an independent flat sequence and assigns a single sequence-level advantage to all tokens, which leads to sample inefficiency and a length bias toward verbose, redundant chains of thought without improving logical depth. We introduce TreeAdv (Tree-Structured Advantage Redistribution for Group-Based RL), which makes the tree structure of group rollouts explicit for both exploration and advantage assignment. Specifically, TreeAdv builds a group of trees (a forest) based on an entropy-driven sampling method where each tree branches at high-uncertainty decisions while sharing low-uncertainty tokens across rollouts. Then, TreeAdv aggregates token-level advantages for internal tree segments by redistributing the advantages of complete rollouts (all leaf nodes), and TreeAdv can easily apply to group-based objectives such as GRPO or GSPO. Across 10 math reasoning benchmarks, TreeAdv consistently outperforms GRPO and GSPO, while using substantially fewer generated tokens under identical supervision, data, and decoding budgets.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reinforcement learning with group-based objectives, such as Group Relative Policy Optimization (GRPO), is a common framework for aligning large language models on complex reasoning tasks.</div>
</details>
</div>
<div class="card">
<div class="title">ADEPT: Adaptive Dynamic Early-Exit Process for Transformers</div>
<div class="meta-line">Authors: Sangmin Yoo, Srikanth Malla, Chiho Choi, Wei D. Lu, Joon Hee Choi</div>
<div class="meta-line">First: 2026-01-07T08:34:41+00:00 · Latest: 2026-01-07T08:34:41+00:00</div>
<div class="meta-line">Comments: 11 figures, 8 tables, 22 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03700v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03700v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The inference of large language models imposes significant computational workloads, often requiring the processing of billions of parameters. Although early-exit strategies have proven effective in reducing computational demands by halting inference earlier, they apply either to only the first token in the generation phase or at the prompt level in the prefill phase. Thus, the Key-Value (KV) cache for skipped layers remains a bottleneck for subsequent token generation, limiting the benefits of early exit. We introduce ADEPT (Adaptive Dynamic Early-exit Process for Transformers), a novel approach designed to overcome this issue and enable dynamic early exit in both the prefill and generation phases. The proposed adaptive token-level early-exit mechanism adjusts computation dynamically based on token complexity, optimizing efficiency without compromising performance. ADEPT further enhances KV generation procedure by decoupling sequential dependencies in skipped layers, making token-level early exit more practical. Experimental results demonstrate that ADEPT improves efficiency by up to 25% in language generation tasks and achieves a 4x speed-up in downstream classification tasks, with up to a 45% improvement in performance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The inference of large language models imposes significant computational workloads, often requiring the processing of billions of parameters.</div>
</details>
</div>
<div class="card">
<div class="title">From Implicit to Explicit: Token-Efficient Logical Supervision for Mathematical Reasoning in LLMs</div>
<div class="meta-line">Authors: Shaojie Wang, Liang Zhang</div>
<div class="meta-line">First: 2026-01-07T08:15:01+00:00 · Latest: 2026-01-07T08:15:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03682v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03682v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent studies reveal that large language models (LLMs) exhibit limited logical reasoning abilities in mathematical problem-solving, instead often relying on pattern-matching and memorization. We systematically analyze this limitation, focusing on logical relationship understanding, which is a core capability underlying genuine logical reasoning, and reveal that errors related to this capability account for over 90\% of incorrect predictions, with Chain-of-Thought Supervised Fine-Tuning (CoT-SFT) failing to substantially reduce these errors. To address this bottleneck, we propose First-Step Logical Reasoning (FSLR), a lightweight training framework targeting logical relationship understanding. Our key insight is that the first planning step-identifying which variables to use and which operation to apply-encourages the model to derive logical relationships directly from the problem statement. By training models on this isolated step, FSLR provides explicit supervision for logical relationship understanding, unlike CoT-SFT which implicitly embeds such relationships within complete solution trajectories. Extensive experiments across multiple models and datasets demonstrate that FSLR consistently outperforms CoT-SFT under both in-distribution and out-of-distribution settings, with average improvements of 3.2\% and 4.6\%, respectively. Moreover, FSLR achieves 4-6x faster training and reduces training token consumption by over 80\%.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent studies reveal that large language models (LLMs) exhibit limited logical reasoning abilities in mathematical problem-solving, instead often relying on pattern-matching and memorization.</div>
</details>
</div>
<div class="card">
<div class="title">Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code</div>
<div class="meta-line">Authors: Jungin Kim, Shinwoo Park, Yo-Sub Han</div>
<div class="meta-line">First: 2025-02-26T05:46:13+00:00 · Latest: 2026-01-07T08:01:06+00:00</div>
<div class="meta-line">Comments: Findings of EACL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.18851v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.18851v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Identifying LLM-generated code through watermarking poses a challenge in preserving functional correctness. Previous methods rely on the assumption that watermarking high-entropy tokens effectively maintains output quality. Our analysis reveals a fundamental limitation of this assumption: syntax-critical tokens such as keywords often exhibit the highest entropy, making existing approaches vulnerable to logic corruption. We present STONE, a syntax-aware watermarking method that embeds watermarks only in non-syntactic tokens and preserves code integrity. For its rigorous assessment, we also introduce STEM, a comprehensive framework that balances three critical dimensions: correctness, detectability, and imperceptibility. Across Python, C++, and Java, STONE preserves correctness, sustains strong detectability, and achieves balanced performance with minimal overhead. Our implementation is available at https://anonymous.4open.science/r/STONE-watermarking-AB4B/.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Identifying LLM-generated code through watermarking poses a challenge in preserving functional correctness.</div>
</details>
</div>
<div class="card">
<div class="title">Table as a Modality for Large Language Models</div>
<div class="meta-line">Authors: Liyao Li, Chao Ye, Wentao Ye, Yifei Sun, Zhe Jiang, Haobo Wang, Jiaming Tian, Yiming Zhang, Ningtao Wang, Xing Fu, Gang Chen, Junbo Zhao</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-11-30T15:59:56+00:00 · Latest: 2026-01-07T07:49:32+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.00947v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.00947v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To migrate the remarkable successes of Large Language Models (LLMs), the community has made numerous efforts to generalize them to the table reasoning tasks for the widely deployed tabular data. Despite that, in this work, by showing a probing experiment on our proposed StructQA benchmark, we postulate that even the most advanced LLMs (such as GPTs) may still fall short of coping with tabular data. More specifically, the current scheme often simply relies on serializing the tabular data, together with the meta information, then inputting them through the LLMs. We argue that the loss of structural information is the root of this shortcoming. In this work, we further propose TAMO, which bears an ideology to treat the tables as an independent modality integrated with the text tokens. The resulting model in TAMO is a multimodal framework consisting of a hypergraph neural network as the global table encoder seamlessly integrated with the mainstream LLM. Empirical results on various benchmarking datasets, including HiTab, WikiTQ, WikiSQL, FeTaQA, and StructQA, have demonstrated significant improvements on generalization with an average relative gain of 42.65%.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To migrate the remarkable successes of Large Language Models (LLMs), the community has made numerous efforts to generalize them to the table reasoning tasks for the widely deployed tabular data.</div>
</details>
</div>
<div class="card">
<div class="title">PhysVideoGenerator: Towards Physically Aware Video Generation via Latent Physics Guidance</div>
<div class="meta-line">Authors: Siddarth Nilol Kundur Satish, Devesh Jaiswal, Hongyu Chen, Abhishek Bakshi</div>
<div class="meta-line">First: 2026-01-07T07:38:58+00:00 · Latest: 2026-01-07T07:38:58+00:00</div>
<div class="meta-line">Comments: 9 pages, 2 figures, project page: https://github.com/CVFall2025-Project/PhysVideoGenerator</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03665v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03665v1">PDF</a> · <a href="https://github.com/CVFall2025-Project/PhysVideoGenerator">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current video generation models produce high-quality aesthetic videos but often struggle to learn representations of real-world physics dynamics, resulting in artifacts such as unnatural object collisions, inconsistent gravity, and temporal flickering. In this work, we propose PhysVideoGenerator, a proof-of-concept framework that explicitly embeds a learnable physics prior into the video generation process. We introduce a lightweight predictor network, PredictorP, which regresses high-level physical features extracted from a pre-trained Video Joint Embedding Predictive Architecture (V-JEPA 2) directly from noisy diffusion latents. These predicted physics tokens are injected into the temporal attention layers of a DiT-based generator (Latte) via a dedicated cross-attention mechanism. Our primary contribution is demonstrating the technical feasibility of this joint training paradigm: we show that diffusion latents contain sufficient information to recover V-JEPA 2 physical representations, and that multi-task optimization remains stable over training. This report documents the architectural design, technical challenges, and validation of training stability, establishing a foundation for future large-scale evaluation of physics-aware generative models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Current video generation models produce high-quality aesthetic videos but often struggle to learn representations of real-world physics dynamics, resulting in artifacts such as unnatural object collisions, inconsistent gravity, and temporal flickering.</div>
</details>
</div>
<div class="card">
<div class="title">TransMamba: A Sequence-Level Hybrid Transformer-Mamba Language Model</div>
<div class="meta-line">Authors: Yixing Li, Ruobing Xie, Zhen Yang, Xingwu Sun, Shuaipeng Li, Weidong Han, Zhanhui Kang, Yu Cheng, Chengzhong Xu, Di Wang, Jie Jiang</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-03-31T13:26:24+00:00 · Latest: 2026-01-07T06:16:53+00:00</div>
<div class="meta-line">Comments: Accepted by AAAI 2026. Code: https://github.com/Yixing-Li/TransMamba</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.24067v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.24067v2">PDF</a> · <a href="https://github.com/Yixing-Li/TransMamba">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformers are the cornerstone of modern large language models, but their quadratic computational complexity limits efficiency in long-sequence processing. Recent advancements in Mamba, a state space model (SSM) with linear complexity, offer promising efficiency gains but suffer from unstable contextual learning and multitask generalization. Some works conduct layer-level hybrid structures that combine Transformer and Mamba layers, aiming to make full use of both advantages. This paper proposes TransMamba, a novel sequence-level hybrid framework that unifies Transformer and Mamba through shared parameter matrices (QKV and CBx), and thus could dynamically switch between attention and SSM mechanisms at different token lengths and layers. We design the Memory Converter to bridge Transformer and Mamba by converting attention outputs into SSM-compatible states, ensuring seamless information flow at TransPoints where the transformation happens. The TransPoint scheduling is also thoroughly explored for balancing effectiveness and efficiency. We conducted extensive experiments demonstrating that TransMamba achieves superior training efficiency and performance compared to single and hybrid baselines, and validated the deeper consistency between Transformer and Mamba paradigms at sequence level, offering a scalable solution for next-generation language modeling. Code and data are available at https://github.com/Yixing-Li/TransMamba</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Transformers are the cornerstone of modern large language models, but their quadratic computational complexity limits efficiency in long-sequence processing.</div>
</details>
</div>
<div class="card">
<div class="title">Back to Basics: Let Denoising Generative Models Denoise</div>
<div class="meta-line">Authors: Tianhong Li, Kaiming He</div>
<div class="meta-line">First: 2025-11-17T18:59:57+00:00 · Latest: 2026-01-07T05:36:57+00:00</div>
<div class="meta-line">Comments: Tech report. Code at https://github.com/LTH14/JiT</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13720v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.13720v2">PDF</a> · <a href="https://github.com/LTH14/JiT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Today&#x27;s denoising diffusion models do not &quot;denoise&quot; in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than &quot;Just image Transformers&quot;, or JiT, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Today&#x27;s denoising diffusion models do not &quot;denoise&quot; in the classical sense, i.e., they do not directly predict clean images.</div>
</details>
</div>
<div class="card">
<div class="title">Policy-Guided Search on Tree-of-Thoughts for Efficient Problem Solving with Bounded Language Model Queries</div>
<div class="meta-line">Authors: Sumedh Pendurkar, Guni Sharon</div>
<div class="meta-line">Venue: Transactions on Machine Learning Research, 2025</div>
<div class="meta-line">First: 2026-01-07T05:35:16+00:00 · Latest: 2026-01-07T05:35:16+00:00</div>
<div class="meta-line">Comments: Published in Transactions on Machine Learning Research (TMLR), 2025. Available at https://openreview.net/forum?id=Rlk1bWe2ii</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03606v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03606v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent studies explored integrating state-space search algorithms with Language Models (LM) to perform look-ahead on the token generation process, the &#x27;&#x27;Tree-of-Thoughts&#x27;&#x27; (ToT), generated by LMs, thereby improving performance on problem-solving tasks. However, the affiliated search algorithms often overlook the significant computational costs associated with LM inference, particularly in scenarios with constrained computational budgets. Consequently, we address the problem of improving LM performance on problem-solving tasks under limited computational budgets. We demonstrate how the probabilities assigned to thoughts by LMs can serve as a heuristic to guide search within the ToT framework, thereby reducing the number of thought evaluations. Building on this insight, we adapt a heuristic search algorithm, Levin Tree Search (LTS), to the ToT framework, which leverages LMs as policies to guide the tree exploration efficiently. We extend the theoretical results of LTS by showing that, for ToT (a pruned tree), LTS guarantees a bound on the number of states expanded, and consequently, on the number of thoughts generated. Additionally, we analyze the sensitivity of this bound to the temperature values commonly used in the final softmax layer of the LM. Empirical evaluation under a fixed LM query budget demonstrates that LTS consistently achieves comparable or higher accuracy than baseline search algorithms within the ToT framework, across three domains (Blocksworld, PrOntoQA, Array Sorting) and four distinct LMs. These findings highlight the efficacy of LTS on ToT, particularly in enabling cost-effective and time-efficient problem-solving, making it well-suited for latency-critical and resource-constrained applications.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent studies explored integrating state-space search algorithms with Language Models (LM) to perform look-ahead on the token generation process, the &#x27;&#x27;Tree-of-Thoughts&#x27;&#x27; (ToT), generated by LMs, thereby improving performance on problem-solving tasks.</div>
</details>
</div>
<div class="card">
<div class="title">ALERT: Zero-shot LLM Jailbreak Detection via Internal Discrepancy Amplification</div>
<div class="meta-line">Authors: Xiao Lin, Philip Li, Zhichen Zeng, Tingwei Li, Tianxin Wei, Xuying Ning, Gaotang Li, Yuzhong Chen, Hanghang Tong</div>
<div class="meta-line">First: 2026-01-07T05:30:53+00:00 · Latest: 2026-01-07T05:30:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03600v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03600v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite rich safety alignment strategies, large language models (LLMs) remain highly susceptible to jailbreak attacks, which compromise safety guardrails and pose serious security risks. Existing detection methods mainly detect jailbreak status relying on jailbreak templates present in the training data. However, few studies address the more realistic and challenging zero-shot jailbreak detection setting, where no jailbreak templates are available during training. This setting better reflects real-world scenarios where new attacks continually emerge and evolve. To address this challenge, we propose a layer-wise, module-wise, and token-wise amplification framework that progressively magnifies internal feature discrepancies between benign and jailbreak prompts. We uncover safety-relevant layers, identify specific modules that inherently encode zero-shot discriminative signals, and localize informative safety tokens. Building upon these insights, we introduce ALERT (Amplification-based Jailbreak Detector), an efficient and effective zero-shot jailbreak detector that introduces two independent yet complementary classifiers on amplified representations. Extensive experiments on three safety benchmarks demonstrate that ALERT achieves consistently strong zero-shot detection performance. Specifically, (i) across all datasets and attack strategies, ALERT reliably ranks among the top two methods, and (ii) it outperforms the second-best baseline by at least 10% in average Accuracy and F1-score, and sometimes by up to 40%.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Despite rich safety alignment strategies, large language models (LLMs) remain highly susceptible to jailbreak attacks, which compromise safety guardrails and pose serious security risks.</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Tool-Integrated Interleaved Thinking towards Cross-Domain Generalization</div>
<div class="meta-line">Authors: Zhengyu Chen, Jinluan Yang, Teng Xiao, Ruochen Zhou, Luan Zhang, Xiangyu Xi, Xiaowei Shi, Wei Wang, Jinggang Wang</div>
<div class="meta-line">First: 2025-10-13T09:19:13+00:00 · Latest: 2026-01-07T04:36:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.11184v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.11184v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) have demonstrated remarkable capabilities in reasoning and tool utilization. However, the generalization of tool-augmented reinforcement learning (RL) across diverse domains remains a significant challenge. Standard paradigms often treat tool usage as a linear or isolated event, which becomes brittle when transferring skills from restricted domains (e.g., mathematics) to open-ended tasks. In this work, we investigate the cross-domain generalization of an LLM agent trained exclusively on mathematical problem-solving. To facilitate robust skill transfer, we propose a {\textbf{R}einforcement Learning for \textbf{I}nterleaved \textbf{T}ool \textbf{E}xecution (RITE)}. Unlike traditional methods, RITE enforces a continuous ``Plan-Action-Reflection&#x27;&#x27; cycle, allowing the model to ground its reasoning in intermediate tool outputs and self-correct during long-horizon tasks. To effectively train this complex interleaved policy, we introduce {Dr. GRPO}, a robust optimization objective that utilizes token-level loss aggregation with importance sampling to mitigate reward sparsity and high-variance credit assignment. Furthermore, we employ a dual-component reward system and dynamic curriculum via online rollout filtering to ensure structural integrity and sample efficiency. Extensive experiments reveal that our approach, despite being trained solely on math tasks, achieves state-of-the-art performance across diverse reasoning domains, demonstrating high token efficiency and strong generalization capabilities.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances in large language models (LLMs) have demonstrated remarkable capabilities in reasoning and tool utilization.</div>
</details>
</div>
<div class="card">
<div class="title">LayerNorm Induces Recency Bias in Transformer Decoders</div>
<div class="meta-line">Authors: Junu Kim, Xiao Liu, Zhenghao Lin, Lei Ji, Yeyun Gong, Edward Choi</div>
<div class="meta-line">First: 2025-09-25T11:48:24+00:00 · Latest: 2026-01-07T04:26:09+00:00</div>
<div class="meta-line">Comments: Codes available at: https://github.com/starmpcc/layernorm_recency_bias</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21042v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.21042v2">PDF</a> · <a href="https://github.com/starmpcc/layernorm_recency_bias">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Causal self-attention provides positional information to Transformer decoders. Prior work has shown that stacks of causal self-attention layers alone induce a positional bias in attention scores toward earlier tokens. However, this differs from the bias toward later tokens typically observed in Transformer decoders, known as recency bias. We address this discrepancy by analyzing the interaction between causal self-attention and other architectural components. We show that stacked causal self-attention layers combined with LayerNorm induce recency bias. Furthermore, we examine the effects of residual connections and the distribution of input token embeddings on this bias. Our results provide new theoretical insights into how positional information interacts with architectural components and suggest directions for improving positional encoding strategies.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Causal self-attention provides positional information to Transformer decoders.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260108_0319.html">20260108_0319</a>
<a href="archive/20260107_0315.html">20260107_0315</a>
<a href="archive/20260106_0319.html">20260106_0319</a>
<a href="archive/20260105_0314.html">20260105_0314</a>
<a href="archive/20260104_0314.html">20260104_0314</a>
<a href="archive/20260103_0313.html">20260103_0313</a>
<a href="archive/20260102_0315.html">20260102_0315</a>
<a href="archive/20260101_0314.html">20260101_0314</a>
<a href="archive/20251231_0315.html">20251231_0315</a>
<a href="archive/20251230_0315.html">20251230_0315</a>
<a href="archive/20251229_0314.html">20251229_0314</a>
<a href="archive/20251228_0313.html">20251228_0313</a>
<a href="archive/20251227_0314.html">20251227_0314</a>
<a href="archive/20251226_0314.html">20251226_0314</a>
<a href="archive/20251225_0314.html">20251225_0314</a>
<a href="archive/20251224_0316.html">20251224_0316</a>
<a href="archive/20251223_0315.html">20251223_0315</a>
<a href="archive/20251222_0314.html">20251222_0314</a>
<a href="archive/20251221_0314.html">20251221_0314</a>
<a href="archive/20251220_0315.html">20251220_0315</a>
<a href="archive/20251219_0317.html">20251219_0317</a>
<a href="archive/20251218_0318.html">20251218_0318</a>
<a href="archive/20251217_0318.html">20251217_0318</a>
<a href="archive/20251216_0318.html">20251216_0318</a>
<a href="archive/20251215_0314.html">20251215_0314</a>
<a href="archive/20251214_0313.html">20251214_0313</a>
<a href="archive/20251213_0315.html">20251213_0315</a>
<a href="archive/20251212_0317.html">20251212_0317</a>
<a href="archive/20251211_0321.html">20251211_0321</a>
<a href="archive/20251210_0318.html">20251210_0318</a>
<a href="archive/20251209_0315.html">20251209_0315</a>
<a href="archive/20251208_0313.html">20251208_0313</a>
<a href="archive/20251207_0314.html">20251207_0314</a>
<a href="archive/20251206_0315.html">20251206_0315</a>
<a href="archive/20251205_0317.html">20251205_0317</a>
<a href="archive/20251203_0317.html">20251203_0317</a>
<a href="archive/20251202_0320.html">20251202_0320</a>
<a href="archive/20251201_0314.html">20251201_0314</a>
<a href="archive/20251130_0313.html">20251130_0313</a>
<a href="archive/20251129_0313.html">20251129_0313</a>
<a href="archive/20251128_0314.html">20251128_0314</a>
<a href="archive/20251127_0314.html">20251127_0314</a>
<a href="archive/20251126_0315.html">20251126_0315</a>
<a href="archive/20251125_0312.html">20251125_0312</a>
<a href="archive/20251124_0313.html">20251124_0313</a>
<a href="archive/20251123_0313.html">20251123_0313</a>
<a href="archive/20251122_0314.html">20251122_0314</a>
<a href="archive/20251121_0314.html">20251121_0314</a>
<a href="archive/20251120_0314.html">20251120_0314</a>
<a href="archive/20251119_0314.html">20251119_0314</a>
<a href="archive/20251118_0313.html">20251118_0313</a>
<a href="archive/20251117_0313.html">20251117_0313</a>
<a href="archive/20251116_0312.html">20251116_0312</a>
<a href="archive/20251115_0314.html">20251115_0314</a>
<a href="archive/20251114_0315.html">20251114_0315</a>
<a href="archive/20251113_0316.html">20251113_0316</a>
<a href="archive/20251112_0315.html">20251112_0315</a>
<a href="archive/20251111_0314.html">20251111_0314</a>
<a href="archive/20251110_0312.html">20251110_0312</a>
<a href="archive/20251109_0313.html">20251109_0313</a>
<a href="archive/20251108_0316.html">20251108_0316</a>
<a href="archive/20251107_0319.html">20251107_0319</a>
<a href="archive/20251106_0316.html">20251106_0316</a>
<a href="archive/20251105_0315.html">20251105_0315</a>
<a href="archive/20251104_0314.html">20251104_0314</a>
<a href="archive/20251103_0313.html">20251103_0313</a>
<a href="archive/20251102_0313.html">20251102_0313</a>
<a href="archive/20251101_0314.html">20251101_0314</a>
<a href="archive/20251031_0314.html">20251031_0314</a>
<a href="archive/20251030_0317.html">20251030_0317</a>
<a href="archive/20251029_0315.html">20251029_0315</a>
<a href="archive/20251028_0316.html">20251028_0316</a>
<a href="archive/20251027_0314.html">20251027_0314</a>
<a href="archive/20251026_0314.html">20251026_0314</a>
<a href="archive/20251025_0313.html">20251025_0313</a>
<a href="archive/20251024_0315.html">20251024_0315</a>
<a href="archive/20251023_0314.html">20251023_0314</a>
<a href="archive/20251022_0317.html">20251022_0317</a>
<a href="archive/20251021_0314.html">20251021_0314</a>
<a href="archive/20251020_0313.html">20251020_0313</a>
<a href="archive/20251019_0312.html">20251019_0312</a>
<a href="archive/20251018_0314.html">20251018_0314</a>
<a href="archive/20251017_0312.html">20251017_0312</a>
<a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
